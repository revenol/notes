{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "Chapter 14. Unsupervised Learning\n",
    "\n",
    "作者竟然把无监督学习放在了一整章节，显示除了一位统计学者的偏好。本次内容比较长，准备两三天读完该章节。另外发现一个问题，当作者以文字叙述、用各种词汇描述解释时，我就看不懂了。这个时候，说明作者也不太精通，做不到“深入浅出”。。。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "### Association Rules\n",
    "\n",
    "1. *Association Rules* 又叫 Market Basket Analysis。嗯，起源于市场分析用户的购物习惯，找到同时购买几件物品的关联性，然后做出推荐、促销。20年前，亚马逊已经用于他们的购物网站了。*Conjunctive rule* 的定义：在所有的 $S_j$ 可能值中，找到一个他的子集 $s_j$，让变量 $X_j$ 在该子集的概率足够大；然后找到所有 p 维子集的一个集合，如果集合的概率相对足够大，则为一个 rule。集合的概率定义如下：\n",
    "    $$\\Pr \\left[  \\bigcap_{j=1}^p (X_j \\in s_j)\\right]$$\n",
    "    相关术语，如果找到一个 association rule：$A \\Rightarrow B$, 即买 A 就很可能买 B，\n",
    "      - antecedent: subset A\n",
    "      - consequent: subset B\n",
    "      - support: $T(A \\Rightarrow B)$，$= \\Pr(A and B)$，即同时购买 A 和 B 的概率\n",
    "      - confidence: $C(A \\Rightarrow B) = \\frac{T(A\\Rightarrow B)}{T(B)}$，即在买 A 的基础上，同时买 B 的概率。这个是最关心的了。\n",
    "      - lift：$L(A \\Rightarrow B) = \\frac{C( A\\Rightarrow B)}{T(B)}$，$=\\frac{\\Pr(A and B)}{\\Pr(A)\\Pr(B)}$，即在购买了 A 的基础上，购买 B 的概率的提升率。如果 lift=1，说明购买 B 与是否购买 A 无关。我们当然是期望 lift>1。\n",
    "\n",
    "1. *Apriori Algorithm* 遍历所有的可能组合，同时通过 hard decision 删除所有概率低于阈值的组合。Apriori 基于一个很强的假设，做个比喻：\n",
    "  > 给定三个变量 A,B 和 C，如果 $\\Pr(ABC)>0.2$，那么必须满足 $\\Pr(AB)>0.2$, $\\Pr(AC)>0.2$ 且 $\\Pr(BC)>0.2$。注意，A, B, C 并不是相互独立的。不然我们就不用找 rule 了。\n",
    "  \n",
    "  Apriori 给定的规则是：\n",
    "  - 集合 $|\\{\\mathcal{K}|{T}(\\mathcal{K}) > t\\}|$ 相对较小。嗯，不然满足条件的集合太多，速度怎么会快呢。\n",
    "  - Any item set $\\mathcal{T}$ consisting of a subset of the items in $\\mathcal{K}$  must have support greater than or equal to that of $\\mathcal{K}$; $\\mathcal{T} \\subseteq \\mathcal{K} \\Rightarrow {T}(\\mathcal{L}) \\geq {T}(\\mathcal{K})$\n",
    "  \n",
    "1. *Unsupervised as Supervised Learning* 这个概念很有意思，在原来无监督的集合 $g(x)$ 上，插入参照集合 $g_0(x)$，然后就可以做 0-1 有监督学习分类了。惨遭的集合可以是 均匀分布、高斯分布、或者与自己本身分布相一致。后面的 *Generalized Association Rules* 基于这个概念，可惜没看懂。以后再说吧。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Analysis\n",
    "\n",
    "1. *Cluster Analysis* 其实主要就是 K-means 算法及其演化算法。\n",
    "  - Proximity Matrices, 可以为 similarities 或者 dissimilarities。通常是个 $N \\times N$ 的矩阵 $\\mathbf{D}$\n",
    "  - Dissimilarities，$D(x_i,x_{i'}) = \\sum_{j=1}^p d_j(x_{ij},x_{i'j})$，其中 $d(x_i,x_{i'})$ 根据变量属性可以为：\n",
    "    - Quantitative variables : $d(x_i,x_{i'}) = l( |x_i -x_{i'}|)$\n",
    "    - Ordinal variables: 比如 A B C D 这种打分机制，$x_i = \\frac{i-1/2}{M}, i = 1,\\cdots,M$\n",
    "    - Categorical variables: 引入相关对称矩阵 $L_{rr'} = L_{r'r} =1$，$L_{rr}=0$\n",
    "    \n",
    "1. *K-means* 算法，就是选择欧氏距离， $d(x_i,x_{i'}) = \\sum_{j=1}^p (x_{ij} - x_{i'j}) = \\|x_i - x_{i'}\\|^2$。此时，有 within-point scatter，就是一个衡量 cluster 内部性质的一个量，类似 loss function 或者 energy function，\n",
    "  \\begin{align}\n",
    "  W(C) =& \\frac{1}{2} \\sum_{k=1}^K \\sum_{C(i)=k}\\sum_{C(i'=k)} \\|x_i - x_{i'}\\|^2 \\\\\n",
    "       =& \\sum_{k=1}^K N_k \\sum_{C(i)=k}\\|x_i - \\bar{x}_k\\|^2 \\\\\n",
    "     N_k =& \\sum_{i=1}^N I(C(i)=k)\n",
    "  \\end{align}\n",
    "  这一步很妙，引入了 mean vector $\\bar{x}_k$ 后，复杂度就少了一个求和，比起直接用 $d(x_i,x_{i'})$，算法复杂度少乘了一个 $N$。\n",
    "  \n",
    "1. *K-medoids* 算法，属于 *K-means* 的演化版本，放弃了上述“很妙的一步”。根本上，\"K-means\" 计算得到一个 Cluster 中心，而 “K-medoids”选择一个数据点为中心。前者计算速度快，后者不容易被某些 outlier 误导。算法也是分两步走，先找中心，再重新分类，然后递归至收敛。\n",
    "\n",
    "1. *K-means* 或者 *K-medoids* 较难确定 K。因为 K 越大，损失函数越小，所以不能用 cross-validation 确定 K。\n",
    "\n",
    "1. *Hierarchical Clustering* 层次聚类，分为自下而上 Agglomerative clustering 和 自上而下 Divisive Clustering 两种。\n",
    "  \n",
    "  其中，Agglomerative clustering 分为：\n",
    "  - *Single linkage* with nearest-neighbor: $d_{SL}(G,H) = \\min_{i\\in G, i'\\in H} d_{ii'}$\n",
    "  - *Complete linkage* with furthest-neighbor: $d_{CL}(G,H) = \\max_{i\\in G, i'\\in H} d(ii')$\n",
    "  - *Group average*: $d_{GA} = \\frac{1}{N_G}\\frac{1}{N_H} \\sum_{i\\in G}\\sum_{i'\\in H}d_{ii'}$\n",
    "  \n",
    "  Divisive clustering 为 从 G 中选择平均相似距离最远的点，划分到 H 中： $\\bar{d}_G = \\frac{1}{N_G} \\sum_{i \\ in  G} \\sum_{i'\\in H} d_{ii'}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Prinicipal Component Analysis\n",
    "\n",
    "1. *Principal Components* 将 p-维的 $x$ 降维到 q-维的 $\\lambda$，其中 $q \\leq p$，其实就是从 X 的 p 个 eigenvalue 中选择 q 个最大的。**注意，这里 eigenvector 会发生变化，并不是从 $X^T = (X_1, X_2,\\cdots,X_p)$ 的 p 个 base 中选择 q 个**。所以，得到的新映射为\n",
    "  \\begin{align}\n",
    "  f(\\lambda) = \\mu + \\mathbf{V}_q \\lambda\n",
    "  \\end{align}\n",
    "  其中 $\\mathbf{V}_q$ 为一个 $p\\times q$ 的矩阵且所有 q 列都正交， $\\lambda$ 是 q 维向量。这两个参数的选择满足近似误差最小，即\n",
    "  \\begin{align}\n",
    "  \\min_{\\mu,\\{\\lambda_i\\},\\mathbf{V}_q} \\sum_{i=1}^N \\| x_i - f(\\lambda_i)\\|^2\n",
    "  \\end{align}\n",
    "  对于给定的正交矩阵 $\\mathbf{V}_q$，上述求解得到\n",
    "  \\begin{align}\n",
    "  \\hat{\\mu} =& \\bar{x} \\\\\n",
    "  \\hat{\\lambda}_i = & \\mathbf{V}_q^T (x_i - \\bar{x})\n",
    "  \\end{align}\n",
    "  不失一般性，我们会假设 $\\hat{\\mu} =\\bar{x} =0$，或者定义 $\\tilde{x}_i = x_i - \\bar{x}$，上述最优化问题可以转化为\n",
    "  \\begin{align}\n",
    "  \\min_{\\mathbf{V}_q} \\sum_{i=1}^N \\| x_i - \\mathbf{V}_q\\mathbf{V}_q^Tx_i\\|^2\n",
    "  \\end{align}\n",
    "    - 其中 $\\mathbf{V}_q\\mathbf{V}_q^T$ 是一个 映射矩阵，先将 $x_i$ 映射到 q 维度 $\\mathbf{V}_q^Tx_i$ ，然后再映射回 p 维度 $\\mathbf{V}_q\\mathbf{V}_q^Tx_i$。信息损失了 $p-q$ 维。\n",
    "    - 求解时需要将 $N \\times p$ 的矩阵 $\\mathbf{X}$ 做 SVD 分解，singular value decomposition，$\\mathbf{X} = \\mathbf{U}\\mathbf{D}\\mathbf{V}^T$，得到\n",
    "      - $\\mathbf{V}_q$ 为 $\\mathbf{V}_q$ 前 q 个最大 eigenvalue 对应的列。\n",
    "      - $\\hat{\\lambda}_i = \\mathbf{U}_q\\mathbf{D}_q$ 为 $\\mathbf{U}\\mathbf{D}$ 的前 q 列。事实上，$\\mathbf{U}\\mathbf{D}$ 的列称为 $\\mathbf{X}$ 的主成分 principal components， PCA 由此而来。\n",
    "      \n",
    "1. *Spectral Clustering*，这个也可以归类到 *K-means clustering*。N 个点之间的 $N \\times N$ 相似矩阵可以表达为一个相似图 similarity graph $G=<V,E>$， 其中 V 代表 N 个 $x_i$， E 为两个点之间的相似概率 $s_{ii'} = \\exp (-d_{ii'}^2/c)$。然后，clustering 就变成了将一个大图分割成几个子图的过程。当然，具体执行的时候，两个子图之间可能存在弱连接，这时候需要判断是连接还是断开。于是，采用降维的方法：\n",
    "  - 定义 graph Laplacian ： $\\mathbf{L} = \\mathbf{G} - \\mathbf{W}$， 其中$\\mathbf{G}$ 满足 $g_i = \\sum_{i'}w_{ii'}$ 是 degree matrix， $\\mathbf{W}$ 是 adjacency matrix 满足对角线都是0。归一化， $\\tilde{\\mathbf{L}} = \\mathbf{I} - \\mathbf{G}^{-1}\\mathbf{W}$.\n",
    "    - 这里 $\\mathbf{G}^{-1}\\mathbf{W}$ 其实是 transition probability matrix，即各个变量 $x_i$ 之间的转移概率。如果考虑一个 Random Walk 过程的话，$x_i$ 只会在它所属的子图 clustering 中走动。\n",
    "  - Spectral analysis，对 $\\tilde{\\mathbf{L}}$ 取最小的 K 个 eigenvalue。等价于取 $\\mathbf{G}^{-1}\\mathbf{W}$ 最大的 K 个 eigenvalue。剩下就可以应用 K-means 算法了。\n",
    "  \n",
    "1. *Kernel Principal Components*。 PCA 对应的是线性的降维，如果数据不是线性的，比如好几个同心环，那么就需要对数据进行预处理，先使用核函数 $\\Theta (\\cdot)$.\n",
    "\n",
    "1. *Non-negative Matrix Factorization* 与 PCA 类似，将 $N \\times p$ 的变量矩阵降维到 $r$，$\\mathbf{X} \\approx \\mathbf{W}\\mathbf{H}$，只是这里的矩阵都是 non-negative 的。\n",
    "\n",
    "1. *Independent Component Analysis, ICA*. $\\mathbf{X} =\\mathbf{U}\\mathbf{D}\\mathbf{V}^T = \\sqrt{N}\\mathbf{U} \\cdot \\mathbf{D}\\mathbf{V}^T/\\sqrt{N} = \\mathbf{S} \\mathbf{A}^T $。 ICA 最小化 components 之间的 mutual information: $I(Y) = \\sum_{j=1}^p H(Y_j) - H(Y)$  \n",
    "  - PCA 的 components 都趋于高斯分布 Gaussian distribution，而 ICA 反其道而行之，让 ICA 越不似 高斯分布，拥有 long-tailed distribution。当 变量之间相互独立的时候，用 ICA。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
