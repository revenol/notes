{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classification\n",
    "_Chapter 4 Linear Methods for Classification_\n",
    "\n",
    "_前一章节介绍了针对 quantitative 变量的线性回归 [Linear Regression](./LinearRegression.ipynb)，本章节讲适用于 qualitative 变量的线性分类 Linear Classification。这两个章节是本书所有内容的基础。根据我的理解，regression 相当于是优化问题中的 linear programming，而 classification则对应 integer programming。根据优化理论，两者的理论基础是共通的，前者如果是 convex problem的话就可以直接求得唯一最优解，后者没有统一的最优解求解方法，并且最优解不唯一。优化问题的很多难题都是在 integer programming 领域，那么对应的，我推测 learning接下来的很多困难都来自classification。_\n",
    "\n",
    "假设有编号为 $1,2,\\cdots,K$ 的 $K$ 个分类，线性模型匹配后得到\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{f}_k(x) = \\hat{\\beta}_{k0}+\\hat{\\beta}_k^Tx.\n",
    "\\end{equation}\n",
    "$$\n",
    "那么决定输入 $x$ 属于 $k$ 还是 $l$ 的分类边界条件为 $\\hat{f}_k(x)=\\hat{f}_l(x)$。该条件是一个 affine 集合或者 hyperplane，即为\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left\\{x:\\left(\\hat{\\beta}_{k0}-\\hat{\\beta}_{l0}\\right)+\\left(\\hat{\\beta}_{k}-\\hat{\\beta}_{l}\\right)^Tx=0\\right\\}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "该分类方法其实是一种回归方法，其为每一类定义判别函数 discriminant functions $\\delta_k(x)$，或者后验概率 posterior probabilities $Pr\\left(G=k|X=x\\right)$(这里分类值用G，以区分回归值Y),然后选择函数值或者概率最大的分类。当判别函数或者后验概率在$x$上是线性的时候，分类的决策边界就是线性的。\n",
    "\n",
    "*边界条件 $\\hat{f}_k(x)=\\hat{f}_l(x)$ 的判别通常有两种方法：减法 $\\hat{f}_k(x)-\\hat{f}_l(x)=0$ 和除法 $\\frac{\\hat{f}_k(x)}{\\hat{f}_l(x)}=1$。在计算上，我们通常偏向于除法（理由以后有兴趣再去考究，感觉是除法可以消除好多的公因子，简化计算）。除法运算后得到的值为 1，通常我们希望和 0 进行比较，于是就引入了对数变换 logit transformation, $\\log{\\frac{\\hat{f}_k(x)}{\\hat{f}_l(x)}}=0$。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内容概览\n",
    "- *Linear Regression*。可以把$G$编码为$K$个 Matrix Indicator $Y_k$，满足仅当$G=k$时$Y_k=1$。直接套用前一章节 linear regression 的方法，求得最佳的匹配\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{f}(x) =& [(1,x^T)\\hat{\\mathbf{B}}]^T \\\\\n",
    "\\hat{\\mathbf{B}} = & (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}\\mathbf{Y}.\n",
    "\\end{align}\n",
    "$$\n",
    "然后，最大的匹配值即为对应的分类\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{G}(x)=\\arg \\max_{k\\in G}\\hat{f}_k(x).\n",
    "\\end{align}\n",
    "$$\n",
    "这个方法显然是行不通的，不然就不用研究 classification了。其本质类似于用 linear programming 的方法来求解 integer programming 的问题，很多情况下得不到最优解。只有小部分的 integer programming 问题可以直接等价与 linear programming 问题，其它的都不可行，甚至连次优解都得不到。\n",
    "\n",
    "- *Linear Discriminant Analysis (LDA)*。基于Bayes定理，\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Pr\\left(G=k|X=x\\right)=\\frac{f_k(x)\\pi_k}{\\sum_{l=1}^{K}f_l(x)\\pi_l}.\n",
    "\\end{align}\n",
    "$$\n",
    "假设$f_k(x)$满足独立标准分布（高斯分布）$(\\mu_k,\\mathbf{\\Sigma})$，\n",
    "$$\n",
    "\\begin{align}\n",
    "f_k(x) = \\frac{1}{(2\\pi)^{p/2}\\left|\\mathbf{\\Sigma}_k\\right|^{1/2}}e^{-\\tfrac{1}{2}(x-\\mu_k)^T\\mathbf{\\Sigma}_k^{-1}(x-\\mu_k)}.\n",
    "\\end{align}\n",
    "$$\n",
    "可以得到线性判别函数\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_k(x)=x^T\\mathbf{\\Sigma}^{-1}\\mu_k - \\tfrac{1}{2}\\mu_k^T\\mathbf{\\Sigma}^{-1}\\mu_k+\\log \\pi_k.\n",
    "\\end{align}\n",
    "$$\n",
    "然后，最大的匹配值即为对应的分类\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{G}(x)=\\arg \\max_{k}\\delta_k(x).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- *Quadratic Discriminant Analysis (QDA)*。与LDA类似，当 $\\mathbf{\\Sigma}_k$ 不相等的时候，可以得到二次判别函数 quadratic discriminant functions\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_k = -\\frac{1}{2}\\log \\left|\\mathbf{\\Sigma}_k\\right| - \\frac{1}{2}(x-\\mu_k)^T\\mathbf{\\Sigma}_k^{-1}(x-\\mu_k) + \\log \\pi_k.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- *Regularized Discriminant Analysis* 是 LDA 和 QDA 的组合，对应的 regularized covariance matrices 为\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{\\Sigma}}_k(\\alpha) = \\alpha \\hat{\\mathbf{\\Sigma}}_k +(1-\\alpha)\\hat{\\mathbf{\\Sigma}},\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\alpha \\in [0,1]$。另外，上式中的 $\\hat{\\mathbf{\\Sigma}}$ 可以进一步 regularized 趋近于单位矩阵,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{\\Sigma}}(\\gamma) = \\gamma \\hat{\\mathbf{\\Sigma}} + (1-\\gamma)\\hat{\\sigma}^2\\hat{\\mathbf{I}},\n",
    "\\end{align}\n",
    "$$\n",
    "for $\\gamma \\in [0,1]$.\n",
    "\n",
    "- *Logistic Regression*。$\\Pr\\left(G=k|X=x\\right)$ 的构造如下，\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Pr\\left(G=k|X=x\\right)=&\\frac{\\exp(\\beta_{k0}+\\beta_k^Tx)}{1+\\sum_{l=1}^{K-1}\\exp(\\beta_{l0}+\\beta_l^Tx)}, \\\\\n",
    "\\Pr\\left(G=K|X=x\\right)=&\\frac{1}{1+\\sum_{l=1}^{K-1}\\exp(\\beta_{l0}+\\beta_l^Tx)}.\n",
    "\\end{align}\n",
    "$$\n",
    "目标是最大化 log-likelihood 函数\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ell (\\beta) = \\sum_{i=1}^{N}\\log \\Pr\\left(G=k|X=x;\\beta\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "最优化的 $\\beta$ 可以通过 Newton-Raphson 算法求得\n",
    "$$\n",
    "\\begin{align}\n",
    "\\beta^{new} = \\beta^{old} - \\left(\\frac{\\partial^2 \\ell (\\beta)}{\\partial \\beta \\partial \\beta^T} \\right)^{-1}\\frac{\\partial \\ell (\\beta)}{\\partial \\beta}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- *$L_1$ Regularized Logistic Regression*。 把之前 Lasso 玩的 $L_1$ penalty 用到了这里，目标是最大化 penalized log-likelihood 函数\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ell (\\beta) = \\sum_{i=1}^{N}\\log \\Pr\\left(G=k|X=x;\\beta\\right) + \\lambda \\sum_{j=1}^{p}\\left|\\beta_j \\right|\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- *Perceptron Learning Algorithm （PLA）*。定义一条线 or Hyperplanes，使错误归类的点到边界的距离总和最小化:\n",
    "$$\n",
    "\\begin{align}\n",
    "D(\\beta, \\beta_0) = - \\sum_{i\\in M}y_i(x_i^T\\beta + \\beta_0).\n",
    "\\end{align}\n",
    "$$\n",
    "其中 $M$ 是错误归类的点集合。算法更新如下\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\\beta \\\\ \\beta_0\\end{pmatrix} \\leftarrow \\begin{pmatrix}\\beta \\\\ \\beta_0\\end{pmatrix} + \\rho \\begin{pmatrix}y_ix_i \\\\ y_i\\end{pmatrix}.\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\rho$ is the learning rate, equaling to 1 by default.\n",
    "\n",
    "- *Optimal Separating Hyperplane*。由于 PLA 的解不唯一，该方法提出最大化任一 class 到 hyperplane 的最小距离。\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\min_{\\beta,\\beta_0} \\frac{1}{2}\\|\\beta\\|^2 \\\\\n",
    "\\textrm{subject to } &y_i(x_i^T\\beta+\\beta_0) \\geq 1, \\ \\ i=1,\\cdots,N.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 具体内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "这里应该介绍下该算法什么时候可行，以及为什么失败。但是我暂时还没弄明白，需要参考其它资料，以后再补充吧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)\n",
    "\n",
    "#### LDA 与 Fisher 算法\n",
    "学过统计的同学会说，LDA就是Fisher算法的特例，增加了正态分布，covariance一致$\\mathbf{\\Sigma}_1=\\mathbf{\\Sigma}_2=\\cdots=\\mathbf{\\Sigma}$且满秩等条件。Fisher Linear Discriminant定义问题为\n",
    "> Find the linear combination $Z=a^T X$ such that the between-class variance is maximized relative to the within-class variance.\n",
    "\n",
    "其数学表达形式为\n",
    "$$\n",
    "\\begin{align}\n",
    "\\max_{a}S=\\frac{\\delta^2_{between}}{\\delta^2_{within}}=\\frac{a^T\\mathbf{B}a}{a^T\\mathbf{W}a},\n",
    "\\end{align}\n",
    "$$\n",
    "其中，$\\mathbf{B}$和 $\\mathbf{W}$分别是 between-class variance 和 within-class variance。另外，$\\mathbf{B}+\\mathbf{W}=\\mathbf{T}$ 就是整个输入 $X$ 的 total covariance matrix。Fisher 的最优化问题等价于\n",
    "$$\n",
    "\\begin{align}\n",
    "\\max_a a^T\\mathbf{B}a \\quad \\textrm{ subject to } \\quad a^T\\mathbf{W}a=1.\n",
    "\\end{align}\n",
    "$$\n",
    "最优解a为$\\mathbf{W}^{-1}\\mathbf{B}$的最大特征值.\n",
    "\n",
    "当$x_k$的 mean 和 variance 分别是 $\\mu_k$ 和 $\\mathbf{\\Sigma}_k$ 时，$Z=a^T X$ 的 mean 和 variance 分别是 $a^T\\mu_k$ 和 $a^T\\mathbf{\\Sigma}_ka$。如果仅有两个变量的话，我们有(参考的[wiki](https://en.wikipedia.org/wiki/Linear_discriminant_analysis))\n",
    "$$\n",
    "\\begin{align}\n",
    "S=\\frac{\\delta^2_{between}}{\\delta^2_{within}}=\\frac{\\left(a^T\\mu_1 - a^T\\mu_2\\right)^2}{a^T\\mathbf{\\Sigma}_1a + a^T\\mathbf{\\Sigma}_2a}=\\frac{\\left(a^T(\\mu_1 -\\mu_2)\\right)^2}{a^T(\\mathbf{\\Sigma}_1+\\mathbf{\\Sigma}_2)a}.\n",
    "\\end{align}\n",
    "$$\n",
    "从而可以得到最优解\n",
    "$$\n",
    "\\begin{align}\n",
    "a \\propto (\\mathbf{\\Sigma}_1+\\mathbf{\\Sigma}_2)^{-1}(\\mu_1 -\\mu_2).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "*对于通信背景的我来说，Fisher的思路其实是最大化 SNR。在通信系统中，输入变量 $x$ 和输出变量 $y$ 的对应关系为 $y=h x$，其中 $h$ 为信道，对应此处的 $a$ 映射。为了成功解调出 $y$ ，其实就是信号分类，对于输入变量$x_1$，$x_2$为干扰噪声，所以我们要使变量自身的 variace (即 $\\mathbf{\\Sigma}_k$ 或者 within-class variance $\\mathbf{W}$) 尽量小，集中在均值点周围，这样$x_2$对应的$y_2$才不会跑到$y_1$的地盘；同时我们尽可能使$y_1$ 和 $y_2$ 相隔足够远，即 between-class variance $\\mathbf{B}$ 最大化，这样在 $y_1$ 和 $y_2$ 之间划一条分割线可以使两个集合的交集最小，误差最小。因此，我们在设计通信系统 $h$ 时，尽力满足正交性，即与 $x_1-x_2$ 正交。*\n",
    "\n",
    "在 Fisher 的算法中，分子 $a^T\\mathbf{B}a$ 比较好理解，为了更好的区分两个类，当然需要让两个类的中心相隔最远，最大化 between-class variance。而分母 $a^T\\mathbf{W}a$ 的解释，见下图4.9。由于每个变量的 within-class variance 不容，变量分布在各个维度上会有倾向性（这个词是我的感官，可能不专业）。如果只考虑分子不除以分母，会得到图4.9左边的图，效果不好；除以分母后，得到右边的图。\n",
    "\n",
    "*我的理解，在除以 within-class variance 后，变量在各个维度上的倾向性被消除了，变成了 variance 为 $\\mathbf{I}$ 的变量。即标准化了*\n",
    "![图4.9](./subfigures/figures4.9.png) <!-- .element height=\"80px\" width=\"50px\" -->\n",
    "\n",
    "#### LDA的计算\n",
    "LDA 包括 QDA 的计算先对角化其 covariance matrix，$\\hat{\\mathbf{\\Sigma}}$ or $\\hat{\\mathbf{\\Sigma}}_k$。\n",
    "\n",
    "- QDA。协方差矩阵对角化后 $\\hat{\\mathbf{\\Sigma}}_k = \\mathbf{U}_k \\mathbf{D}_k \\mathbf{U}_k^T$，其中 $\\mathbf{U}_k$ 是 $p\\times p$ 的正定矩阵 orthonormal matrix；$\\mathbf{D}_k$ 是对角矩阵，令其特征根为 $d_{kl}$。判别函数的计算分以下两个部分\n",
    "$$\n",
    "\\begin{align}\n",
    "(x-\\mu_k)^T\\hat{\\mathbf{\\Sigma}}_k^{-1}(x-\\mu_k) = & [\\mathbf{\\Sigma}_k^T(x-\\hat{\\mu}_k)]^T \\mathbf{D}_k^{-1} [\\mathbf{\\Sigma}_k^T(x-\\hat{\\mu}_k)] \\\\\n",
    "\\log \\left|\\hat{\\mathbf{\\Sigma}}_k\\right| =& \\sum_{l} \\log d_{kl}\n",
    "\\end{align}\n",
    "$$\n",
    "在实际计算过程中，先计算 $\\mathbf{\\Sigma}_k^T(x-\\hat{\\mu}_k)$ 部分。\n",
    "\n",
    "- LDA。与 QDA 类似，对角化协方差矩阵 $\\hat{\\mathbf{\\Sigma}} = \\mathbf{U} \\mathbf{D}\\mathbf{U}^T$。可以对输入 $\\mathbf{X}$ 进行映射处理： $\\mathbf{X}^* \\leftarrow \\mathbf{D}^{\\tfrac{1}{2}}\\mathbf{U}^T\\mathbf{X}$，新得到的 $\\mathbf{X}^*$ 的协方差矩阵是个单位矩阵。\n",
    "\n",
    "#### LDA 参数估计\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\pi}_k =& \\frac{N_k}{N} \\\\\n",
    "\\hat{\\mu}_k =& \\sum_{g_i=k}\\frac{x_i}{N_k} \\\\\n",
    "\\hat{\\mathbf{\\Sigma}} =& \\sum_{k=1}^K\\sum_{g_i=k}\\frac{(x_i - \\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^T}{N-K}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (logit)\n",
    "*由于 Linear Regression 简称为 LR，那么这个就叫 logit 吧，参考的其他博客。*\n",
    "\n",
    "以后验概率为例，为了保证 logit 之后的边界条件满足线性要求，\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log{\\frac{\\Pr\\left(G=k|X=x\\right)}{\\Pr\\left(G=K|X=x\\right)}}=\\beta_{k0}+\\beta_k^Tx,\n",
    "\\end{align}\n",
    "$$\n",
    "通常会对概率进行指数化处理，$Pr\\left(G=k|X=x\\right)=\\exp(\\beta_{k0}+\\beta_k^Tx)$，然后归一化得到\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Pr\\left(G=k|X=x\\right)=&\\frac{\\exp(\\beta_{k0}+\\beta_k^Tx)}{\\sum_{l=1}^{K}\\exp(\\beta_{l0}+\\beta_l^Tx)}.\n",
    "\\end{align}\n",
    "$$\n",
    "如果任意选取一组为参照组，默认为第 $K$ 组，考虑归一化后得到\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Pr\\left(G=k|X=x\\right)=&\\frac{\\exp(\\beta_{k0}+\\beta_k^Tx)}{1+\\sum_{l=1}^{K-1}\\exp(\\beta_{l0}+\\beta_l^Tx)}, \\\\\n",
    "\\Pr\\left(G=K|X=x\\right)=&\\frac{1}{1+\\sum_{l=1}^{K-1}\\exp(\\beta_{l0}+\\beta_l^Tx)}.\n",
    "\\end{align}\n",
    "$$\n",
    "目标是最大化 log-likelihood 函数\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ell (\\theta) = \\sum_{i=1}^{N}\\log p_{g_i}(x_i;\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "where $p_k(x_i;\\theta)=\\Pr\\left(G=k|X=x;\\theta\\right)$。\n",
    "\n",
    "以 two-class 为例。为什么是 two？虽然思路是一样，算法还是可以实现，但是多了就太复杂了，没办法写出 expression。令当 $G_i = 1$ ($G_i = 2$) 时 $y_i = 1$ ($y_i = 0$)。同时，令 $p_1(x;\\beta)= \\frac{\\exp{\\beta^Tx}}{1+\\exp{\\beta^Tx}} = p(x;\\beta)$，那么 $p_2(x;\\beta)= 1- p(x;\\beta)$。我们可以得到 log-likelihood 函数\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ell{\\beta} = \\sum_{i=1}^{N}\\left\\{y_i\\log p(x_i;\\beta) + (1-y_i)\\log (1-p(x_i;\\beta))  \\right\\} = \\sum_{i=1}^{N}\\left\\{y_i \\beta^T x_i - \\log (1+ e^{\\beta^Tx_i})  \\right\\} \n",
    "\\end{align}\n",
    "$$\n",
    "一阶导数，或者 *score* equation 为\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\ell (\\beta)}{\\partial \\beta} = \\sum_{i=1}^{N}x_i(y_i-p(x_i;\\beta)) =0\n",
    "\\end{align}\n",
    "$$\n",
    "二阶导数，或者 *Hessian* matrix 为\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial^2 \\ell (\\beta)}{\\partial \\beta \\partial \\beta^T} = -\\sum_{i=1}^{N}x_i x_i^Tp(x_i;\\beta)(1-p(x_i;\\beta)) =0.\n",
    "\\end{align}\n",
    "$$\n",
    "通过 Newton-Raphson 算法求得\n",
    "$$\n",
    "\\begin{align}\n",
    "\\beta^{new} = \\beta^{old} - \\left(\\frac{\\partial^2 \\ell (\\beta)}{\\partial \\beta \\partial \\beta^T} \\right)^{-1}\\frac{\\partial \\ell (\\beta)}{\\partial \\beta}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "将以上推到写成矩阵形式，\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\ell (\\beta)}{\\partial \\beta} =  & \\mathbf{X}^T(\\mathbf{y} - \\mathbf{p}) \\\\\n",
    "\\frac{\\partial^2 \\ell (\\beta)}{\\partial \\beta \\partial \\beta^T} =& -\\mathbf{X}^T\\mathbf{W}\\mathbf{X}\n",
    "\\end{align}\n",
    "$$\n",
    "其中 $\\mathbf{X}$ 是 $N\\times(p+1)$ 维矩阵，\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{P} =& \\left[p(x_0;\\beta),\\ p(x_1;\\beta),\\ p(x_2;\\beta),\\ \\cdots,\\ p(x_N;\\beta)  \\right]^T \\\\\n",
    "\\mathbf{W} =& diag \\left[p(x_0;\\beta)(1-p(x_0;\\beta)) \\quad p(x_1;\\beta)(1-p(x_1;\\beta)) \\quad p(x_2;\\beta)(1-p(x_2;\\beta))\\quad \\cdots \\quad p(x_N;\\beta)(1-p(x_N;\\beta)) \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "最后牛顿迭代可以写成\n",
    "$$\n",
    "\\begin{align}\n",
    "\\beta^{new} = \\beta^{old} + (\\mathbf{X}^T\\mathbf{W}\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{y}-\\mathbf{p}) = (\\mathbf{X}^T\\mathbf{W}\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{W}\\mathbf{z}\n",
    "\\end{align}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{z} = \\mathbf{X} \\beta^{old} + \\mathbf{W}^{-1}(\\mathbf{y}-\\mathbf{p}).\n",
    "\\end{align}\n",
    "$$\n",
    "上面的迭代算法被称为 *iteratively reweighted least squares* (IRLS)，因为每一步相当于求解一个 weighted least squares 问题：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\beta^{new}\\leftarrow \\arg \\min_{\\beta} (\\mathbf{z} - \\mathbf{X}\\beta )^T\\mathbf{W} (\\mathbf{z} - \\mathbf{X}\\beta ).\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LDA vs. logit\n",
    "\n",
    "#### 结论：\n",
    "logit 要求的条件更少，适应范围更广；LDA 基于 X 的高斯分布假设，对模型了解更多，预测效率更高。\n",
    "\n",
    "#### 理由：\n",
    "- Outlier, logit 胜: logistic regression 是一种回归，即分布在边缘的变量会显得不那么重要；LDA 中每一个变量都会影响协方差函数 $\\mathbf{\\Sigma}$。\n",
    "- Performance：对于一般的分布，LDR 和 logit 都能获得类似的性能。但是，当变量满足高斯分布时，LDR 最优且复杂度低。但是但是，当不知道变量分布的情况下，还是用LR吧，因为附加条件少，可以适配如上述 outlier 的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Hyperplanes\n",
    "该方法依旧是基于线性条件，定义 Hyperplane or *affine set* $L$ 为\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x) = \\beta_0 + \\beta^T x = 0.\n",
    "\\end{align}\n",
    "$$\n",
    "基于向量原理，任意一点 $x$ 到 $L$ 的有向距离为\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\beta^T}{\\|\\beta\\|} (x-x_0) = \\frac{1}{\\|\\beta\\|} (\\beta^T x+\\beta_0) = \\frac{1}{\\|f'(x)\\|}f(x)  \n",
    "\\end{align}\n",
    "$$\n",
    "where $x_0$ is any point in $L$ and it satisfies\n",
    "$$\n",
    "\\begin{align}\n",
    "\\beta^T x_0 = -\\beta_0.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Rosenblatt's Perceptron Learning Algorithm\n",
    "Rosenblatt 在 1958 年提出的算法，让错误归类的点到 $L$ 的距离最小。该方法常用于神经网络分析，对滴，就是信号传输走最短距离。\n",
    "$$\n",
    "\\begin{align}\n",
    "D(\\beta, \\beta_0) = - \\sum_{i\\in M}y_i(x_i^T\\beta + \\beta_0).\n",
    "\\end{align}\n",
    "$$\n",
    "其中 $M$ 是错误归类的点集合。\n",
    "\n",
    "分别对 $\\beta$ 和 $\\beta_0$ 求导，然后采用随机梯度下降法 *stochastic gradient descent* 求解。\n",
    "$$\n",
    "\\begin{align}\n",
    "\\partial \\frac{D(\\beta, \\beta_0)}{\\partial \\beta} =& - \\sum_{i\\in M}y_ix_i\\\\\n",
    "\\partial \\frac{D(\\beta, \\beta_0)}{\\partial \\beta_0} =& - \\sum_{i\\in M}y_i\n",
    "\\end{align}\n",
    "$$\n",
    "算法更新如下\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\\beta \\\\ \\beta_0\\end{pmatrix} \\leftarrow \\begin{pmatrix}\\beta \\\\ \\beta_0\\end{pmatrix} + \\rho \\begin{pmatrix}y_ix_i \\\\ y_i\\end{pmatrix}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "该方法存在3个问题(Ripley, 1996):\n",
    "- 当输入变量可以分离的时候，因为最小距离都为0，存在很多的可行解。(*见下一个 optimal perceptron learning algorithm*)\n",
    "- 虽然随机梯度下降法可以在有限步骤内得到最优解，但是迭代次数可能非常高。（*这个问题最易解决，比如设置不同的初始值、learning rate $\\rho$ 等方法*）\n",
    "- 当数据不可以分离的时候，迭代算法不收敛。（*这个要命，无解。更高级的 support vector machine 可以解决部分问题*）\n",
    "\n",
    "#### Optimal Perceptron Learning Algorithm\n",
    "解决了 PLA 算法答案不唯一的问题。最大化任意一点到 Hyperplane 的最小距离\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\max_{\\beta,\\beta_0, \\|\\beta\\|=1} M \\\\\n",
    "\\textrm{subject to } &y_i(x_i^T\\beta+\\beta_0) \\geq M, \\ \\ i=1,\\cdots,N.\n",
    "\\end{align}\n",
    "$$\n",
    "由于 $\\beta$ 的 scale 可以调整，问题等价于\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\min_{\\beta,\\beta_0} \\frac{1}{2}\\|\\beta\\|^2 \\\\\n",
    "\\textrm{subject to } &y_i(x_i^T\\beta+\\beta_0) \\geq 1, \\ \\ i=1,\\cdots,N.\n",
    "\\end{align}\n",
    "$$\n",
    "该 convex 优化问题对应的拉格朗日函数为，\n",
    "$$\n",
    "\\begin{align}\n",
    "L_p = \\frac{1}{2}\\|\\beta\\|^2 - \\sum_{i=1}^{N} \\alpha_i[y_i(x_i^T\\beta+\\beta_0) -1].\n",
    "\\end{align}\n",
    "$$\n",
    "对 $\\beta$ 和 $\\beta_0$求导可得\n",
    "$$\n",
    "\\begin{align}\n",
    "\\beta =& \\sum_{i=1}^{N} \\alpha_i y_ix_i \\\\\n",
    "0 =& \\sum_{i=1}^{N} \\alpha_i y_i\n",
    "\\end{align}\n",
    "$$\n",
    "同时，由 KKT 条件得最优解满足 \n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha_i[y_i(x_i^T\\beta+\\beta_0) -1] = 0.\n",
    "\\end{align}\n",
    "$$\n",
    "要么 $\\alpha_i = 0$；或者 $y_i(x_i^T\\beta+\\beta_0) =1$，即 $x_i$ 在 $L$ 的平行线上。我们在比较下 $\\beta$ 的公式，有意思了，出去了所有 $\\alpha_i = 0$ 的相式， $\\beta$ 是由所有这些在 $L$ 的平行线上的 $x_i$ 决定的。这些 $x_i$ 被称为 *support points*。可以相信，这个概念和以后的 support vector machine 有关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "LDA，logit，和 PLA 都是线性分类算法，大家强调不同输入变量。\n",
    "- LDA 受所有输入变量影响，甚至是 outlier。LDA 测算的是 variance，outlier 对 variance 的影响较大。\n",
    "- logit 对输入变量做回归，outlier的影响比较小。logit 测算的是 mean or sum，outlier 的影响被稀释了。\n",
    "- PLA 关注边界上的变量，即 support points。\n",
    "\n",
    "根据不同的分布，可以选择不同的分类算法。\n",
    "\n",
    "*如果预先不知道输入分布？我觉得 logit 更适合。因为 logit 和贝叶斯概率函数有关。即使我们不知道输入变量的分布，但是只要数据量足够大，一定符合大数定理，概率就准了。*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
