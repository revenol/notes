{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classification\n",
    "_Chapter 4 Linear Methods for Classification_\n",
    "\n",
    "_前一章节介绍了针对 quantitative 变量的 [Linear Regression](./LinearRegression.ipynb)，本章节讲适用于 qualitative 变量的 Linear Classification。这两个章节是本书所有内容的基础。根据我的理解，regression 相当于是优化问题中的 linear programming，而 classification则对应 integer programming。根据优化理论，两者的理论基础是共通的，前者如果是 convex problem的话就可以直接求得唯一最优解，后者没有统一的最优解求解方法，并且最优解不唯一。优化问题的很多难题都是在 integer programming 领域，那么对应的，我推测 learning接下来的很多困难都来自classification。_\n",
    "\n",
    "假设有编号为$1,2,\\cdots,K$的$K$个分类，线性模型匹配后得到\n",
    "\\begin{align}\n",
    "\\hat{f}_k(x) = \\hat{\\beta}_{k0}+\\hat{\\beta}_k^Tx.\n",
    "\\end{align}\n",
    "那么决定输入$x$属于$k$还是$l$的分类边界条件为$\\hat{f}_k(x)=\\hat{f}_l(x)$。该条件是一个affine集合或者hyperplane，即为\n",
    "\\begin{align}\n",
    "\\left\\{x:\\left(\\hat{\\beta}_{k0}-\\hat{\\beta}_{l0}\\right)+\\left(\\hat{\\beta}_{k}-\\hat{\\beta}_{l}\\right)^Tx=0\\right\\}.\n",
    "\\end{align}\n",
    "\n",
    "该分类方法其实是一种回归方法，其为每一类定义判别函数 discriminant functions $\\delta_k(x)$，或者后验概率 posterior probabilities $Pr\\left(G=k|X=x\\right)$(这里分类值用G，以区分回归值Y),然后选择函数值或者概率最大的分类。当判别函数或者后验概率在$x$上是线性的时候，分类的决策边界就是线性的。\n",
    "\n",
    "*边界条件$\\hat{f}_k(x)=\\hat{f}_l(x)$的判别通常有两种方法：减法$\\hat{f}_k(x)-\\hat{f}_l(x)=0$和除法$\\frac{\\hat{f}_k(x)}{\\hat{f}_l(x)}=1$。在计算上，我们通常偏向于除法（理由以后有兴趣再去考究，感觉是除法可以消除好多的公因子，简化计算）。除法运算后得到的值为1，通常我们希望和0进行比较，于是就引入了对数变换 logit transformation, $\\log{\\frac{\\hat{f}_k(x)}{\\hat{f}_l(x)}}=0$。*\n",
    "\n",
    "以后验概率为例，为了保证 logit 之后的边界条件满足线性要求，\n",
    "\\begin{align}\n",
    "\\log{\\frac{Pr\\left(G=k|X=x\\right)}{Pr\\left(G=K|X=x\\right)}}=\\beta_{k0}+\\beta_k^Tx,\n",
    "\\end{align}\n",
    "通常会对概率进行指数化处理，$Pr\\left(G=k|X=x\\right)=\\exp(\\beta_{k0}+\\beta_k^Tx)$，然后归一化得到\n",
    "\\begin{align}\n",
    "Pr\\left(G=k|X=x\\right)=&\\frac{\\exp(\\beta_{k0}+\\beta_k^Tx)}{1+\\sum_{l=1}^{K-1}\\exp(\\beta_{l0}+\\beta_l^Tx)}, \\\\\n",
    "Pr\\left(G=K|X=x\\right)=&\\frac{1}{1+\\sum_{l=1}^{K-1}\\exp(\\beta_{l0}+\\beta_l^Tx)}.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "## 内容概览\n",
    "- Linear Regression。可以把$G$编码为$K$个 Matrix Indicator $Y_k$，满足仅当$G=k$时$Y_k=1$。直接套用前一章节 linear regression 的方法，求得最佳的匹配\n",
    "\\begin{align}\n",
    "\\hat{f}(x) =& [(1,x^T)\\hat{\\mathbf{B}}]^T \\\\\n",
    "\\hat{\\mathbf{B}} = & (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}\\mathbf{Y}.\n",
    "\\end{align}\n",
    "然后，最大的匹配值即为对应的分类\n",
    "\\begin{align}\n",
    "\\hat{G}(x)=\\arg \\max_{k\\in G}\\hat{f}_k(x).\n",
    "\\end{align}\n",
    "这个方法显然是行不通的，不然就不用研究 classification了。其本质类似于用 linear programming 的方法来求解 integer programming 的问题，很多情况下得不到最优解。只有小部分的 integer programming 问题可以直接等价与 linear programming 问题，其它的都不可行，甚至连次优解都得不到。\n",
    "\n",
    "- Linear Discriminant Analysis (LDA)。基于Bayes定理，\n",
    "\\begin{align}\n",
    "Pr\\left(G=k|X=x\\right)=\\frac{f_k(x)\\pi_k}{\\sum_{l=1}^{K}f_l(x)\\pi_l}.\n",
    "\\end{align}\n",
    "假设$f_k(x)$满足独立标准分布（高斯分布）$(\\mu_k,\\mathbf{\\Sigma})$，可以得到线性判别函数\n",
    "\\begin{align}\n",
    "\\delta_k(x)=x^T\\mathbf{\\Sigma}^{-1}\\mu_k - \\tfrac{1}{2}\\mu_k^T\\mathbf{\\Sigma}^{-1}\\mu_k+\\log \\pi_k.\n",
    "\\end{align}\n",
    "然后，最大的匹配值即为对应的分类\n",
    "\\begin{align}\n",
    "\\hat{G}(x)=\\arg \\max_{k}\\delta_k(x).\n",
    "\\end{align}\n",
    "\n",
    "## 具体内容\n",
    "\n",
    "### Linear Regression\n",
    "这里应该介绍下该算法什么时候可行，以及为什么失败。但是我暂时还没弄明白，需要参考其它资料，以后再补充吧。\n",
    "\n",
    "### Linear Discriminant Analysis (LDA)\n",
    "学过统计的同学会说，LDA就是Fisher算法的特例，增加了正态分布，covariance一致$\\mathbf{\\Sigma}_1=\\mathbf{\\Sigma}_2=\\cdots=\\mathbf{\\Sigma}$且满秩等条件。Fisher Linear Discriminant定义问题为\n",
    "> Find the linear combination $Z=a^T X$ such that the between-class variance is maximized relative to the within-class variance.\n",
    "\n",
    "其数学表达形式为\n",
    "\\begin{align}\n",
    "\\max_{a}S=\\frac{\\delta^2_{between}}{\\delta^2_{within}}=\\frac{a^T\\mathbf{B}a}{a^T\\mathbf{W}a},\n",
    "\\end{align}\n",
    "其中，$\\mathbf{B}$和 $\\mathbf{W}$分别是 between-class variance 和 within-class variance。另外，$\\mathbf{B}+\\mathbf{W}=\\mathbf{T}$ 就是整个输入 $X$ 的 total covariance matrix。Fisher 的最优化问题等价于\n",
    "\\begin{align}\n",
    "\\max_a a^T\\mathbf{B}a \\quad \\textrm{ subject to } \\quad a^T\\mathbf{W}a=1.\n",
    "\\end{align}\n",
    "最优解a为$\\mathbf{W}^{-1}\\mathbf{B}$的最大特征值.\n",
    "\n",
    "当$x_k$的 mean 和 variance 分别是 $\\mu_k$ 和 $\\mathbf{\\Sigma}_k$ 时，$Z=a^T X$ 的 mean 和 variance 分别是 $a^T\\mu_k$ 和 $a^T\\mathbf{\\Sigma}_ka$。如果仅有两个变量的话，我们有(参考的[wiki](https://en.wikipedia.org/wiki/Linear_discriminant_analysis))\n",
    "\\begin{align}\n",
    "S=\\frac{\\delta^2_{between}}{\\delta^2_{within}}=\\frac{\\left(a^T\\mu_1 - a^T\\mu_2\\right)^2}{a^T\\mathbf{\\Sigma}_1a + a^T\\mathbf{\\Sigma}_2a}=\\frac{\\left(a^T(\\mu_1 -\\mu_2)\\right)^2}{a^T(\\mathbf{\\Sigma}_1+\\mathbf{\\Sigma}_2)a}.\n",
    "\\end{align}\n",
    "从而可以得到最优解\n",
    "\\begin{align}\n",
    "a \\propto (\\mathbf{\\Sigma}_1+\\mathbf{\\Sigma}_2)^{-1}(\\mu_1 -\\mu_2).\n",
    "\\end{align}\n",
    "\n",
    "*对于通信背景的我来说，Fisher的思路其实是最大化SNR。在通信系统中，输入变量 $x$ 和输出变量 $y$ 的对应关系为 $y=h x$，其中 $h$ 为信道，对应此处的 $a$ 映射。为了成功解调出 $y$ ，其实就是信号分类，对于输入变量$x_1$，$x_2$为干扰噪声，所以我们要使变量自身的 variace (即 $\\mathbf{\\Sigma}_k$ 或者 within-class variance $\\mathbf{W}$) 尽量小，集中在均值点周围，这样$x_2$对应的$y_2$才不会跑到$y_1$的地盘；同时我们尽可能使$y_1$ 和 $y_2$ 相隔足够远，即 between-class variance $\\mathbf{B}$ 最大化，这样在 $y_1$ 和 $y_2$ 之间划一条分割线可以使两个集合的交集最小，误差最小。因此，我们在设计通信系统 $h$ 时，尽力满足正交性，即与 $x_1-x_2$ 正交。*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
