{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "\n",
    "Chapter 13. Prototypes and Nearest-Neighbors\n",
    "\n",
    "本章节接着 SVM 讲分类，主要是 K-means 算法 和 KNN 算法。不同于 SVM 在数学上的严谨，本次介绍的算法更似 黑盒子：\n",
    "  > as *black box* prediction engines, they can be very effective, and are often among the best performers in real data problems.\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. *Prototype Methods* 原型方法。在特征集合中，放置好多 prototype，每个 prototype 对应一个分类标签，然后每个待分类的变量就属于离它最近的 prototype 的类。prototype 函数可以为 mean，Gaussian...\n",
    "\n",
    "1. *K-means Clustering* 这个基本代表了 prototype methods，即把数据分为 K 类，每一类可以放置 R 个 prototype。\n",
    "  - 在数据集上随机放置 $K\\times R$ 个prototype\n",
    "  - 识别每个数据所属于的类别，必为 K 中的一个，然后就近分配到该类别的 R 个 prototype 中的一个\n",
    "  - 更新 prototype 的位置，对属于该 prototype 内的数据求平均\n",
    "  - 重复上述操作至收敛\n",
    "  \n",
    "1. *Learning Vector Quantization* LVQ。通常我们假设同类别的数据会汇聚在一起，相当于有一个“重心”。但是 K-means 算法中 prototype 的放置是完全均匀的，某一类的 prototype 放置与另外一个类毫不相干，没有人为干预。这会有一个问题，当两个类的点重合较多时，两个不同类的 prototype 会非常近，这会导致更高的 分类错误。LVQ 就是在 K-means 的基础上，人为调整 prototype 的位置，让两个不同类别的 prototype 相隔远一些，相同类别的 prototype 向自己的“重心”汇聚。\n",
    "  - 假设 $m_j(k)$ 是离 $x_i$ 最近的 prototype，如果 $x_i$ 属于该 $k$ 类，者该 prototype 向 $x_i$ 靠近\n",
    "  $$ m_j(k) \\leftarrow m_j(k) + \\epsilon (x_i - m_j(k))$$\n",
    "  - 如果 $x_i$ 不属于该 $k$ 类，者该 prototype 远离 $x_i$ \n",
    "  $$ m_j(k) \\leftarrow m_j(k) - \\epsilon (x_i - m_j(k))$$\n",
    "  - 重复上述操作，并且每次减小学习率 learning rate $\\epsilon$。由于每个 $x_i$ 都是 vector，再加上 learning rate，LVQ 名字由此而来。\n",
    "  \n",
    "1. *Gaussian Mixtures* 把求平均换成 Guassian mixtures。好吧，我已经对作者到处卖弄 Guassian 产生厌恶了。\n",
    "\n",
    "1. *K-Nearest-Neighbor* KNN 算法。简单到不能再简单，每个变量的分类由它最近的 K 个邻居投票决定。\n",
    "\n",
    "1. *Discriminant Adaptive Nearest-Neighbor* DANN. 为了弥补 KNN 在高位空间的缺陷提出的改进算法，有条件的选取 K 个邻居。如果是个 p 维的空间，每个维度不再是均匀扩展，即现在不再是在一个 p 维的类似球的空间选择了，而是在一个椭球形上选择。即不同维度的 p 上的距离不同的权重，这就是 discriminant，相关的类别维度上的距离权重就会高一些。举个极端的例子，如果一个类别的判定只与一种维度相关，那么就可以把 p 维空间压缩到一维。DANN 是局部的维度调整，可以根据不同的数据点位置，得到 球形 或者 椭球形。\n",
    "  \\begin{align}\n",
    "  D(x,x_0) =& (x-x_0)^T \\mathbf{\\Sigma} (x - x_0) \\\\\n",
    "  \\mathbf{\\Sigma} = &\\mathbf{W}^{-1/2}[\\mathbf{W}^{-1/2}\\mathbf{B}\\mathbf{W}^{-1/2}+ \\epsilon \\mathbf{I}]\\mathbf{W}^{-1/2}\n",
    "  \\end{align}\n",
    "  where $\\mathbf{W}$ is the pooled within-class covariance matrix $\\sum_{k=1}^K \\pi_k \\mathbf{W}_k$ and $\\mathbf{B}$ is the between class covariance matrix $\\sum_{k=1}^K \\pi_k (\\bar{x}_k - \\bar{x})(\\bar{x}_k - \\bar{x})^T$. 更多细节在 [Chapter 4.3.1](./LinearClassification.ipynb) 有介绍。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
