{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Elements of Statistical Learning (ESL) 学习笔记\n",
    "该笔记主要记录个人在学习ESL的历程，包括心得体会、参考资料、程序实现等。\n",
    "\n",
    "- 关于书籍：我买的是世界图书出版社的影印版，中文名称《统计学习基础（第2版）》。\n",
    "\n",
    "- Stanford官方网站：[The Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/) 作者们把书籍的PDF版也放在网站上了，很是自信。经典书籍，看完PDF，你还是会想买一本收藏的。上面也有一些最新课本的勘误。\n",
    "\n",
    "- 有人整理了课本的[课后习题答案](http://waxworksmath.com/Authors/G_M/Hastie/WriteUp/weatherwax_epstein_hastie_solutions_manual.pdf)：\n",
    "\n",
    "- 复旦大学计算机学院吴立德教授以该书为教材开设课程，名为《统计学习精要》。另有牛人Liyun Chen旁听该课程，整理出[听课笔记]( http://www.loyhome.com/elements_of_statistical_learining_lecture_notes/) 。在自学该课程时，可以看到旁人的观点，受益匪浅。\n",
    "笔记中会出现好多中英文混搭，有人喜有人恶。我选择用英文表达专有名词，起到特别强调的作用；另外，也是英文对统计学习领域不熟，好多翻译不明确，不如直接照抄书本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学前状态\n",
    "通信背景，没有Machine Learning，研究queueing，对Probability比较熟悉，对Markov链比较熟悉。\n",
    "\n",
    "### 学习动机\n",
    "- 当AlphaGo2.0战胜了柯洁第一局的时候，我自负地说不意外：距离AlphaGo1.0已经大半年了，Google可以把硬件计算能力翻倍或者更强。但是，当发现这次AlphaGo2.0的硬件计算能力（1个TPU）比1.0（50个TPU）时低时，我被震惊到了。原来，计算机并不仅仅是依靠硬件计算能力，它可以自我学习。\n",
    "- Android来临的时候，我错过了app开发；Cloud来临的时候，我错过了MapReduce。这次AI来临了，我希望可以赶上。\n",
    "- AI 的学习研究，需要一个系统性的基础，于是我选择了ESL。任何领域的深入研究，都需要一个夯实的基础，能透过现象看到本质，做到知其所以然，不然只能做到模仿，不具备创新性。之后可以选择性的看某些相关书籍，然后相互对应，整理出自己的理解逻辑。\n",
    "\n",
    "\n",
    "### 学习计划\n",
    "- ESL成为了我的AI入门书籍。根据介绍，ESL的作者们在Statistics，尤其是Linear Model方面讲得深入浅出（尽管深入浅出这个词现在已经被用烂）。作者们介绍最前沿的Machine Leaning算法，能清楚解释其与最基本的Least Squares算法的共性与不同，并给出算法的设计思路与原理，实在难得。因此，计划详细阅读书中与Linear Model相关的内容，尤其是作者推荐的第1-4、7章节，浅略翻看Bayesian Methods、Neural Networks、Support Vector Machines (SVM)、Graphical Models等章节。\n",
    "- 动手实线部分算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习过程\n",
    "1. Introduction to the book.\n",
    "\n",
    "1. Overview of Supervised Learning ([Chapter 2](./Overview.ipynb))\n",
    "\n",
    "1. Quantitative linear regression([Chapter 3](./LinearRegression.ipynb))\n",
    "\n",
    "1. Qualitative linear classification ([Chapter 4](./LinearClassification.ipynb))\n",
    "\n",
    "1. Basis Expansions and Splines ([Chapter 5](./Splines.ipynb))\n",
    "\n",
    "1. Kernel Smoothing Methods ([Chapter 6](./KernelSmoothing.ipynb))\n",
    "\n",
    "1. Model Assessment and Selection ([Chapter 7](./ModelAssessment.ipynb))\n",
    "\n",
    "1. Model Inference and Averaging ([Chapter 8](./ModelInference.ipynb))\n",
    "\n",
    "1. Additive Models, Trees, and Related Methods ([Chapter 9](./AdditiveModel.ipynb))\n",
    "\n",
    "1. Boosting and Additive Trees ([Chpater 10](./BoostingTree.ipynb))\n",
    "\n",
    "1. Random Forests ([Chapter 15](./RandomForest.ipynb))\n",
    "\n",
    "1. Neural Networks ([Chpater 11](./NeuralNetwork.ipynb))\n",
    "\n",
    "1. Support Vector Machines and Flexible Discriminants ([Chapter 12](./SVM.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
