{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "Chapter 14. Unsupervised Learning\n",
    "\n",
    "作者竟然把无监督学习放在了一整章节，显示除了一位统计学者的偏好。本次内容比较长，准备两三天读完该章节。另外发现一个问题，当作者以文字叙述、用各种词汇描述解释时，我就看不懂了。这个时候，说明作者也不太精通，做不到“深入浅出”。。。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. *Association Rules* 又叫 Market Basket Analysis。嗯，起源于市场分析用户的购物习惯，找到同时购买几件物品的关联性，然后做出推荐、促销。20年前，亚马逊已经用于他们的购物网站了。*Conjunctive rule* 的定义：在所有的 $S_j$ 可能值中，找到一个他的子集 $s_j$，让变量 $X_j$ 在该子集的概率足够大；然后找到所有 p 维子集的一个集合，如果集合的概率相对足够大，则为一个 rule。集合的概率定义如下：\n",
    "    $$\\Pr \\left[  \\bigcap_{j=1}^p (X_j \\in s_j)\\right]$$\n",
    "    相关术语，如果找到一个 association rule：$A \\Rightarrow B$, 即买 A 就很可能买 B，\n",
    "      - antecedent: subset A\n",
    "      - consequent: subset B\n",
    "      - support: $T(A \\Rightarrow B)$，$= \\Pr(A and B)$，即同时购买 A 和 B 的概率\n",
    "      - confidence: $C(A \\Rightarrow B) = \\frac{T(A\\Rightarrow B)}{T(B)}$，即在买 A 的基础上，同时买 B 的概率。这个是最关心的了。\n",
    "      - lift：$L(A \\Rightarrow B) = \\frac{C( A\\Rightarrow B)}{T(B)}$，$=\\frac{\\Pr(A and B)}{\\Pr(A)\\Pr(B)}$，即在购买了 A 的基础上，购买 B 的概率的提升率。如果 lift=1，说明购买 B 与是否购买 A 无关。我们当然是期望 lift>1。\n",
    "\n",
    "1. *Apriori Algorithm* 遍历所有的可能组合，同时通过 hard decision 删除所有概率低于阈值的组合。Apriori 基于一个很强的假设，做个比喻：\n",
    "  > 给定三个变量 A,B 和 C，如果 $\\Pr(ABC)>0.2$，那么必须满足 $\\Pr(AB)>0.2$, $\\Pr(AC)>0.2$ 且 $\\Pr(BC)>0.2$。注意，A, B, C 并不是相互独立的。不然我们就不用找 rule 了。\n",
    "  \n",
    "  Apriori 给定的规则是：\n",
    "  - 集合 $|\\{\\mathcal{K}|{T}(\\mathcal{K}) > t\\}|$ 相对较小。嗯，不然满足条件的集合太多，速度怎么会快呢。\n",
    "  - Any item set $\\mathcal{T}$ consisting of a subset of the items in $\\mathcal{K}$  must have support greater than or equal to that of $\\mathcal{K}$; $\\mathcal{T} \\subseteq \\mathcal{K} \\Rightarrow {T}(\\mathcal{L}) \\geq {T}(\\mathcal{K})$\n",
    "  \n",
    "1. *Unsupervised as Supervised Learning* 这个概念很有意思，在原来无监督的集合 $g(x)$ 上，插入参照集合 $g_0(x)$，然后就可以做 0-1 有监督学习分类了。惨遭的集合可以是 均匀分布、高斯分布、或者与自己本身分布相一致。后面的 *Generalized Association Rules* 基于这个概念，可惜没看懂。以后再说吧。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *Cluster Analysis* 其实主要就是 K-means 算法及其演化算法。\n",
    "  - Proximity Matrices, 可以为 similarities 或者 dissimilarities。通常是个 $N \\times N$ 的矩阵 $\\mathbf{D}$\n",
    "  - Dissimilarities，$D(x_i,x_{i'}) = \\sum_{j=1}^p d_j(x_{ij},x_{i'j})$，其中 $d(x_i,x_{i'})$ 根据变量属性可以为：\n",
    "    - Quantitative variables : $d(x_i,x_{i'}) = l( |x_i -x_{i'}|)$\n",
    "    - Ordinal variables: 比如 A B C D 这种打分机制，$x_i = \\frac{i-1/2}{M}, i = 1,\\cdots,M$\n",
    "    - Categorical variables: 引入相关对称矩阵 $L_{rr'} = L_{r'r} =1$，$L_{rr}=0$\n",
    "    \n",
    "1. *K-means* 算法，就是选择欧氏距离， $d(x_i,x_{i'}) = \\sum_{j=1}^p (x_{ij} - x_{i'j}) = \\|x_i - x_{i'}\\|^2$。此时，有 within-point scatter，就是一个衡量 cluster 内部性质的一个量，类似 loss function 或者 energy function，\n",
    "  \\begin{align}\n",
    "  W(C) =& \\frac{1}{2} \\sum_{k=1}^K \\sum_{C(i)=k}\\sum_{C(i'=k)} \\|x_i - x_{i'}\\|^2 \\\\\n",
    "       =& \\sum_{k=1}^K N_k \\sum_{C(i)=k}\\|x_i - \\bar{x}_k\\|^2 \\\\\n",
    "     N_k =& \\sum_{i=1}^N I(C(i)=k)\n",
    "  \\end{align}\n",
    "  这一步很妙，引入了 mean vector $\\bar{x}_k$ 后，复杂度就少了一个求和，比起直接用 $d(x_i,x_{i'})$，算法复杂度少乘了一个 $N$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
