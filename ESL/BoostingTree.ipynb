{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Trees\n",
    "\n",
    "Chapter 10. Boosting and Additive Trees\n",
    "\n",
    "本章节先介绍了 Boosting 方法，经典的 AdaBoost.M1 算法；然后作者把它与 Tree 相结合，卖自己的 GTBA 算法，甚至把 Boosting 捧上了天：\n",
    "> AdaBoost with trees as the \"best off-the-shelf classifier in the world\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. *Boosting* ，用一些弱分类器 (weak classifier) $G_m(x)$，加权迭代操作，最终实现优异的二元分类效果。\n",
    "\\begin{align}\n",
    "G(x) = sign\\left(\\sum_{m=1}^M \\alpha_mG_m(x)\\right).\n",
    "\\end{align}\n",
    "可以有 Adaboost.M1 算法实现。\n",
    "\n",
    "1. *Additive model* Boosting 和 Additive model 在原理上是相通的\n",
    "\\begin{align}\n",
    "f(x) = \\sum_{m=1}^M \\beta_m b(x;\\gamma_m)\n",
    "\\end{align}\n",
    "在最小化损失函数时，提出向前分布算法 (Forward Stagewise Additive Modeling)\n",
    "\\begin{align}\n",
    "\\min_{\\{\\beta_m,\\gamma_m\\}_1^M} \\sum_{i=1}^N L \\left(y_i, \\sum_{m=1}^M \\beta_m b(x_i;\\gamma_m)  \\right).\n",
    "\\end{align}\n",
    "\n",
    "1. *Boosting Trees* 就是把树\n",
    "\\begin{align}\n",
    "T(x; \\Theta) = & \\sum_{j=1}^J \\gamma_j I(x \\in R_j) \\\\\n",
    "\\Theta = & \\{ R_j, \\gamma_j \\}_1^J\n",
    "\\end{align}\n",
    "带入上上述的 Additive model 中\n",
    "\\begin{align}\n",
    "f_M(x) = \\sum_{m=1}^M T(x; \\Theta)\n",
    "\\end{align}\n",
    "注意，上述公式每个数前面没有权重，因为权重已经隐藏在了 $\\Theta$ 中。\n",
    "\n",
    "1. *Gradient Tree Boosting Algorithm* 梯度回归数提升算法，用于解上述每个 Boosting Tree。关于该算法的使用：\n",
    "  - $4 \\leq J \\leq 8$ 就够了。\n",
    "  - Shrinkage ,$v$, 可以降低错误率。\n",
    "  - 在 Shrinkage 基础上，Subsampling ,$\\eta$, 可以进一步提高性能。 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 具体内容\n",
    "\n",
    "### AdaBoosting.M1.\n",
    "\n",
    "若分类器的定义：\n",
    "> A weak classifier is one whose error rate is only slightly better than random guessing\n",
    "\n",
    "Boosting 把握住了弱分类器只对部分数据有效，那么把所有的弱分类器 $G_m(x)$ 叠加，若可以覆盖所有数据，那就成功了。先给所有训练数据一些初始权重 $w_i = 1/N $，在每个弱分类器后，成功则降低权重，错误则增加权重，以期待被后面的弱分类器成功分类.\n",
    "\\begin{align}\n",
    "err_m =& \\frac{\\sum_{i=1}^N w_i I(y_i \\neq G_m(x_i))}{\\sum_{i=1}^Nw_i} \\\\\n",
    "\\alpha_m = & \\log((1-err_m)/err_m)\\\\\n",
    "w_i \\leftarrow & w_i \\cdot exp[\\alpha_m \\cdot I(y_i \\neq G_m(x_i))]\n",
    "\\end{align}\n",
    "\n",
    "### Forward Stagewise Additive Modeling\n",
    "\n",
    "也是计算每个基函数的最优化参数，然后叠加。\n",
    "\\begin{align}\n",
    "(\\beta_m, \\gamma_m) = & \\arg \\min_{\\beta, \\gamma} \\sum_{i=1}^N L(y_i, f_{m-1}(x_i) + \\beta b(x_i;\\gamma)) \\\\\n",
    "f_m(x) = & f_{m-1}(x) + \\beta_m b(x; \\gamma_m)\n",
    "\\end{align}\n",
    "\n",
    "### Forward Stagewise Boosted Tree Modeling\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\Theta}_m = \\arg \\min_{\\Theta_m}\\sum_{i=1}^N L(y_i, f_{m-1}(x_i)+T(x_i;\\Theta_m))\n",
    "\\end{align}\n",
    "通常直接求解两个优化变量非常困难，可以分部进行，反正模型也不完全需要最优：\n",
    "1. Finding $\\gamma_{jm}$ given $R_{jm}$.\n",
    "1. Finding $R_{jm}$\n",
    "\n",
    "可以对比上面的 Forward Stagewise Additive Modeling，同样是两个优化参数。\n",
    "\n",
    "\n",
    "### Gradient Tree Boosting Algorithm\n",
    "\n",
    "先看传统的梯度下降法，对于目标函数\n",
    "\\begin{align}\n",
    "L(f) = \\sum_{i=1}^N L(y_i,f(x_i))\n",
    "\\end{align}\n",
    "每次沿着梯度方向递减\n",
    "\\begin{align}\n",
    "g_{im} =& \\left[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\right]_{f(x_i) = f_{m-1}(x_i)} \\\\\n",
    "\\rho_m =& \\arg \\min_{\\rho} L(\\mathbf{f}_{m-1} -\\rho \\mathbf{g}_m) \\\\\n",
    "\\mathbf{f}_m = & \\mathbf{f}_{m-1} -\\rho_m \\mathbf{g}_m\n",
    "\\end{align}\n",
    "\n",
    "Boosting Tree 有些不同，每次函数的加减必须是一颗树，而不是简单的梯度。这两个的维度不一样，在完全树的情况下，可以等价。但是，Boosting Tree 通常是简单的不完全生成树，所以需要一些近似的操作。方法就是生产一颗新的树 $T_m(x;\\Theta)$，使它拟合所有的 梯度 $r_{im}$。\n",
    "\\begin{align}\n",
    "r_{im} =& -\\left[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\right]_{f(x_i) = f_{m-1}(x_i)} \\\\\n",
    "T_m(x;\\Theta) \\sim  r_{im} \\Rightarrow & R_{jm} \\\\\n",
    "\\gamma_{jm} = & \\arg \\min_{\\gamma} \\sum_{x_i \\in R_{jm}} L(y_i, f_{m-1}(x_i) + \\gamma) \\\\\n",
    "f_m(x) =& f_{m-1}(x) + T_m(x;\\Theta=(R_{jm},\\gamma_{jm}))\n",
    "\\end{align}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
