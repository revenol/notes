{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classification\n",
    "_Chapter 4 Linear Methods for Classification_\n",
    "\n",
    "_前一章节介绍了针对 quantitative 变量的线性回归 [Linear Regression](./LinearRegression.ipynb)，本章节讲适用于 qualitative 变量的线性分类 Linear Classification。这两个章节是本书所有内容的基础。根据我的理解，regression 相当于是优化问题中的 linear programming，而 classification则对应 integer programming。根据优化理论，两者的理论基础是共通的，前者如果是 convex problem的话就可以直接求得唯一最优解，后者没有统一的最优解求解方法，并且最优解不唯一。优化问题的很多难题都是在 integer programming 领域，那么对应的，我推测 learning接下来的很多困难都来自classification。_\n",
    "\n",
    "假设有编号为 $1,2,\\cdots,K$ 的 $K$ 个分类，线性模型匹配后得到\n",
    "\\begin{equation}\n",
    "\\hat{f}_k(x) = \\hat{\\beta}_{k0}+\\hat{\\beta}_k^Tx.\n",
    "\\end{equation}\n",
    "那么决定输入 $x$ 属于 $k$ 还是 $l$ 的分类边界条件为 $\\hat{f}_k(x)=\\hat{f}_l(x)$。该条件是一个 affine 集合或者 hyperplane，即为\n",
    "\\begin{align}\n",
    "\\left\\{x:\\left(\\hat{\\beta}_{k0}-\\hat{\\beta}_{l0}\\right)+\\left(\\hat{\\beta}_{k}-\\hat{\\beta}_{l}\\right)^Tx=0\\right\\}.\n",
    "\\end{align}\n",
    "\n",
    "该分类方法其实是一种回归方法，其为每一类定义判别函数 discriminant functions $\\delta_k(x)$，或者后验概率 posterior probabilities $Pr\\left(G=k|X=x\\right)$(这里分类值用G，以区分回归值Y),然后选择函数值或者概率最大的分类。当判别函数或者后验概率在$x$上是线性的时候，分类的决策边界就是线性的。\n",
    "\n",
    "*边界条件 $\\hat{f}_k(x)=\\hat{f}_l(x)$ 的判别通常有两种方法：减法 $\\hat{f}_k(x)-\\hat{f}_l(x)=0$ 和除法 $\\frac{\\hat{f}_k(x)}{\\hat{f}_l(x)}=1$。在计算上，我们通常偏向于除法（理由以后有兴趣再去考究，感觉是除法可以消除好多的公因子，简化计算）。除法运算后得到的值为 1，通常我们希望和 0 进行比较，于是就引入了对数变换 logit transformation, $\\log{\\frac{\\hat{f}_k(x)}{\\hat{f}_l(x)}}=0$。*\n",
    "\n",
    "\n",
    "\n",
    "## 内容概览\n",
    "- Linear Regression。可以把$G$编码为$K$个 Matrix Indicator $Y_k$，满足仅当$G=k$时$Y_k=1$。直接套用前一章节 linear regression 的方法，求得最佳的匹配\n",
    "\\begin{align}\n",
    "\\hat{f}(x) =& [(1,x^T)\\hat{\\mathbf{B}}]^T \\\\\n",
    "\\hat{\\mathbf{B}} = & (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}\\mathbf{Y}.\n",
    "\\end{align}\n",
    "然后，最大的匹配值即为对应的分类\n",
    "\\begin{align}\n",
    "\\hat{G}(x)=\\arg \\max_{k\\in G}\\hat{f}_k(x).\n",
    "\\end{align}\n",
    "这个方法显然是行不通的，不然就不用研究 classification了。其本质类似于用 linear programming 的方法来求解 integer programming 的问题，很多情况下得不到最优解。只有小部分的 integer programming 问题可以直接等价与 linear programming 问题，其它的都不可行，甚至连次优解都得不到。\n",
    "\n",
    "- Linear Discriminant Analysis (LDA)。基于Bayes定理，\n",
    "\\begin{align}\n",
    "\\Pr\\left(G=k|X=x\\right)=\\frac{f_k(x)\\pi_k}{\\sum_{l=1}^{K}f_l(x)\\pi_l}.\n",
    "\\end{align}\n",
    "假设$f_k(x)$满足独立标准分布（高斯分布）$(\\mu_k,\\mathbf{\\Sigma})$，可以得到线性判别函数\n",
    "\\begin{align}\n",
    "\\delta_k(x)=x^T\\mathbf{\\Sigma}^{-1}\\mu_k - \\tfrac{1}{2}\\mu_k^T\\mathbf{\\Sigma}^{-1}\\mu_k+\\log \\pi_k.\n",
    "\\end{align}\n",
    "然后，最大的匹配值即为对应的分类\n",
    "\\begin{align}\n",
    "\\hat{G}(x)=\\arg \\max_{k}\\delta_k(x).\n",
    "\\end{align}\n",
    "\n",
    "- Quadratic Discriminant Analysis (QDA)。与LDA类似，当 $\\mathbf{\\Sigma}_k$ 不相等的时候，可以得到二次判别函数 quadratic discriminant functions\n",
    "\\begin{align}\n",
    "\\delta_k = -\\frac{1}{2}\\log \\left|\\mathbf{\\Sigma}_k\\right| - \\frac{1}{2}(x-\\mu_k)^T\\mathbf{\\Sigma}_k^{-1}(x-\\mu_k) + \\log \\pi_k.\n",
    "\\end{align}\n",
    "\n",
    "- Regularized Discriminant Analysis 是 LDA 和 QDA 的组合，对应的 regularized covariance matrices 为\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{\\Sigma}}_k(\\alpha) = \\alpha \\hat{\\mathbf{\\Sigma}}_k +(1-\\alpha)\\hat{\\mathbf{\\Sigma}},\n",
    "\\end{align}\n",
    "where $\\alpha \\in [0,1]$。另外，上式中的 $\\hat{\\mathbf{\\Sigma}}$ 可以进一步 regularized 趋近于单位矩阵,\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{\\Sigma}}(\\gamma) = \\gamma \\hat{\\mathbf{\\Sigma}} + (1-\\gamma)\\hat{\\sigma}^2\\hat{\\mathbf{I}},\n",
    "\\end{align}\n",
    "for $\\gamma \\in [0,1]$.\n",
    "\n",
    "- Logistic Regression。$\\Pr\\left(G=k|X=x\\right)$ 的构造如下，\n",
    "\\begin{align}\n",
    "\\Pr\\left(G=k|X=x\\right)=&\\frac{\\exp(\\beta_{k0}+\\beta_k^Tx)}{1+\\sum_{l=1}^{K-1}\\exp(\\beta_{l0}+\\beta_l^Tx)}, \\\\\n",
    "\\Pr\\left(G=K|X=x\\right)=&\\frac{1}{1+\\sum_{l=1}^{K-1}\\exp(\\beta_{l0}+\\beta_l^Tx)}.\n",
    "\\end{align}\n",
    "目标是最大化 log-likelihood 函数\n",
    "\\begin{align}\n",
    "\\ell (\\beta) = \\sum_{i=1}^{N}\\log \\Pr\\left(G=k|X=x;\\beta\\right)\n",
    "\\end{align}\n",
    "最优化的 $\\beta$ 可以通过 Newton-Raphson 算法求得\n",
    "\\begin{align}\n",
    "\\beta^{new} = \\beta^{old} - \\left(\\frac{\\partial^2 \\ell (\\beta)}{\\partial \\beta \\partial \\beta^T} \\right)^{-1}\\frac{\\partial \\ell (\\beta)}{\\partial \\beta}\n",
    "\\end{align}\n",
    "\n",
    "## 具体内容\n",
    "\n",
    "### Linear Regression\n",
    "这里应该介绍下该算法什么时候可行，以及为什么失败。但是我暂时还没弄明白，需要参考其它资料，以后再补充吧。\n",
    "\n",
    "### Linear Discriminant Analysis (LDA)\n",
    "\n",
    "#### LDA 与 Fisher 算法\n",
    "学过统计的同学会说，LDA就是Fisher算法的特例，增加了正态分布，covariance一致$\\mathbf{\\Sigma}_1=\\mathbf{\\Sigma}_2=\\cdots=\\mathbf{\\Sigma}$且满秩等条件。Fisher Linear Discriminant定义问题为\n",
    "> Find the linear combination $Z=a^T X$ such that the between-class variance is maximized relative to the within-class variance.\n",
    "\n",
    "其数学表达形式为\n",
    "\\begin{align}\n",
    "\\max_{a}S=\\frac{\\delta^2_{between}}{\\delta^2_{within}}=\\frac{a^T\\mathbf{B}a}{a^T\\mathbf{W}a},\n",
    "\\end{align}\n",
    "其中，$\\mathbf{B}$和 $\\mathbf{W}$分别是 between-class variance 和 within-class variance。另外，$\\mathbf{B}+\\mathbf{W}=\\mathbf{T}$ 就是整个输入 $X$ 的 total covariance matrix。Fisher 的最优化问题等价于\n",
    "\\begin{align}\n",
    "\\max_a a^T\\mathbf{B}a \\quad \\textrm{ subject to } \\quad a^T\\mathbf{W}a=1.\n",
    "\\end{align}\n",
    "最优解a为$\\mathbf{W}^{-1}\\mathbf{B}$的最大特征值.\n",
    "\n",
    "当$x_k$的 mean 和 variance 分别是 $\\mu_k$ 和 $\\mathbf{\\Sigma}_k$ 时，$Z=a^T X$ 的 mean 和 variance 分别是 $a^T\\mu_k$ 和 $a^T\\mathbf{\\Sigma}_ka$。如果仅有两个变量的话，我们有(参考的[wiki](https://en.wikipedia.org/wiki/Linear_discriminant_analysis))\n",
    "\\begin{align}\n",
    "S=\\frac{\\delta^2_{between}}{\\delta^2_{within}}=\\frac{\\left(a^T\\mu_1 - a^T\\mu_2\\right)^2}{a^T\\mathbf{\\Sigma}_1a + a^T\\mathbf{\\Sigma}_2a}=\\frac{\\left(a^T(\\mu_1 -\\mu_2)\\right)^2}{a^T(\\mathbf{\\Sigma}_1+\\mathbf{\\Sigma}_2)a}.\n",
    "\\end{align}\n",
    "从而可以得到最优解\n",
    "\\begin{align}\n",
    "a \\propto (\\mathbf{\\Sigma}_1+\\mathbf{\\Sigma}_2)^{-1}(\\mu_1 -\\mu_2).\n",
    "\\end{align}\n",
    "\n",
    "*对于通信背景的我来说，Fisher的思路其实是最大化 SNR。在通信系统中，输入变量 $x$ 和输出变量 $y$ 的对应关系为 $y=h x$，其中 $h$ 为信道，对应此处的 $a$ 映射。为了成功解调出 $y$ ，其实就是信号分类，对于输入变量$x_1$，$x_2$为干扰噪声，所以我们要使变量自身的 variace (即 $\\mathbf{\\Sigma}_k$ 或者 within-class variance $\\mathbf{W}$) 尽量小，集中在均值点周围，这样$x_2$对应的$y_2$才不会跑到$y_1$的地盘；同时我们尽可能使$y_1$ 和 $y_2$ 相隔足够远，即 between-class variance $\\mathbf{B}$ 最大化，这样在 $y_1$ 和 $y_2$ 之间划一条分割线可以使两个集合的交集最小，误差最小。因此，我们在设计通信系统 $h$ 时，尽力满足正交性，即与 $x_1-x_2$ 正交。*\n",
    "\n",
    "在 Fisher 的算法中，分子 $a^T\\mathbf{B}a$ 比较好理解，为了更好的区分两个类，当然需要让两个类的中心相隔最远，最大化 between-class variance。而分母 $a^T\\mathbf{W}a$ 的解释，见下图4.9。由于每个变量的 within-class variance 不容，变量分布在各个维度上会有倾向性（这个词是我的感官，可能不专业）。如果只考虑分子不除以分母，会得到图4.9左边的图，效果不好；除以分母后，得到右边的图。\n",
    "\n",
    "*我的理解，在除以 within-class variance 后，变量在各个维度上的倾向性被消除了，变成了 variance 为 $\\mathbf{I}$ 的变量。即标准化了*\n",
    "![图4.9](./subfigures/figures4.9.png) <!-- .element height=\"80px\" width=\"50px\" -->\n",
    "\n",
    "### LDA的计算\n",
    "LDA 包括 QDA 的计算先对角化其 covariance matrix，$\\hat{\\mathbf{\\Sigma}}$ or $\\hat{\\mathbf{\\Sigma}}_k$。\n",
    "\n",
    "- QDA。协方差矩阵对角化后 $\\hat{\\mathbf{\\Sigma}}_k = \\mathbf{U}_k \\mathbf{D}_k \\mathbf{U}_k^T$，其中 $\\mathbf{U}_k$ 是 $p\\times p$ 的正定矩阵 orthonormal matrix；$\\mathbf{D}_k$ 是对角矩阵，令其特征根为 $d_{kl}$。判别函数的计算分以下两个部分\n",
    "\\begin{align}\n",
    "(x-\\mu_k)^T\\hat{\\mathbf{\\Sigma}}_k^{-1}(x-\\mu_k) = & [\\mathbf{\\Sigma}_k^T(x-\\hat{\\mu}_k)]^T \\mathbf{D}_k^{-1} [\\mathbf{\\Sigma}_k^T(x-\\hat{\\mu}_k)] \\\\\n",
    "\\log \\left|\\hat{\\mathbf{\\Sigma}}_k\\right| =& \\sum_{l} \\log d_{kl}\n",
    "\\end{align}\n",
    "在实际计算过程中，先计算 $\\mathbf{\\Sigma}_k^T(x-\\hat{\\mu}_k)$ 部分。\n",
    "\n",
    "- LDA。与 QDA 类似，对角化协方差矩阵 $\\hat{\\mathbf{\\Sigma}} = \\mathbf{U} \\mathbf{D}\\mathbf{U}^T$。可以对输入 $\\mathbf{X}$ 进行映射处理： $\\mathbf{X}^* \\leftarrow \\mathbf{D}^{\\tfrac{1}{2}}\\mathbf{U}^T\\mathbf{X}$，新得到的 $\\mathbf{X}^*$ 的协方差矩阵是个单位矩阵。\n",
    "\n",
    "### Logistic Regression (Logit)\n",
    "以后验概率为例，为了保证 logit 之后的边界条件满足线性要求，\n",
    "\\begin{align}\n",
    "\\log{\\frac{\\Pr\\left(G=k|X=x\\right)}{\\Pr\\left(G=K|X=x\\right)}}=\\beta_{k0}+\\beta_k^Tx,\n",
    "\\end{align}\n",
    "通常会对概率进行指数化处理，$Pr\\left(G=k|X=x\\right)=\\exp(\\beta_{k0}+\\beta_k^Tx)$，然后归一化得到\n",
    "\\begin{align}\n",
    "\\Pr\\left(G=k|X=x\\right)=&\\frac{\\exp(\\beta_{k0}+\\beta_k^Tx)}{\\sum_{l=1}^{K}\\exp(\\beta_{l0}+\\beta_l^Tx)}.\n",
    "\\end{align}\n",
    "如果任意选取一组为参照组，默认为第 $K$ 组，考虑归一化后得到\n",
    "\\begin{align}\n",
    "\\Pr\\left(G=k|X=x\\right)=&\\frac{\\exp(\\beta_{k0}+\\beta_k^Tx)}{1+\\sum_{l=1}^{K-1}\\exp(\\beta_{l0}+\\beta_l^Tx)}, \\\\\n",
    "\\Pr\\left(G=K|X=x\\right)=&\\frac{1}{1+\\sum_{l=1}^{K-1}\\exp(\\beta_{l0}+\\beta_l^Tx)}.\n",
    "\\end{align}\n",
    "目标是最大化 log-likelihood 函数\n",
    "\\begin{align}\n",
    "\\ell (\\theta) = \\sum_{i=1}^{N}\\log p_{g_i}(x_i;\\theta)\n",
    "\\end{align}\n",
    "where $p_k(x_i;\\theta)=\\Pr\\left(G=k|X=x;\\theta\\right)$。\n",
    "\n",
    "以 two-class 为例。为什么是 two？虽然思路是一样，算法还是可以实现，但是多了就太复杂了，没办法写出 expression。令当 $G_i = 1$ ($G_i = 2$) 时 $y_i = 1$ ($y_i = 0$)。同时，令 $p_1(x;\\beta)= \\frac{\\exp{\\beta^Tx}}{1+\\exp{\\beta^Tx}} = p(x;\\beta)$，那么 $p_2(x;\\beta)= 1- p(x;\\beta)$。我们可以得到 log-likelihood 函数\n",
    "\\begin{align}\n",
    "\\ell{\\beta} = \\sum_{i=1}^{N}\\left\\{y_i\\log p(x_i;\\beta) + (1-y_i)\\log (1-p(x_i;\\beta))  \\right\\} = \\sum_{i=1}^{N}\\left\\{y_i \\beta^T x_i - \\log (1+ e^{\\beta^Tx_i})  \\right\\} \n",
    "\\end{align}\n",
    "一阶导数，或者 *score* equation 为\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\ell (\\beta)}{\\partial \\beta} = \\sum_{i=1}^{N}x_i(y_i-p(x_i;\\beta)) =0\n",
    "\\end{align}\n",
    "二阶导数，或者 *Hessian* matrix 为\n",
    "\\begin{align}\n",
    "\\frac{\\partial^2 \\ell (\\beta)}{\\partial \\beta \\partial \\beta^T} = -\\sum_{i=1}^{N}x_i x_i^Tp(x_i;\\beta)(1-p(x_i;\\beta)) =0.\n",
    "\\end{align}\n",
    "通过 Newton-Raphson 算法求得\n",
    "\\begin{align}\n",
    "\\beta^{new} = \\beta^{old} - \\left(\\frac{\\partial^2 \\ell (\\beta)}{\\partial \\beta \\partial \\beta^T} \\right)^{-1}\\frac{\\partial \\ell (\\beta)}{\\partial \\beta}\n",
    "\\end{align}\n",
    "\n",
    "将以上推到写成矩阵形式，\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\ell (\\beta)}{\\partial \\beta} =  & \\mathbf{X}^T(\\mathbf{y} - \\mathbf{p}) \\\\\n",
    "\\frac{\\partial^2 \\ell (\\beta)}{\\partial \\beta \\partial \\beta^T} =& -\\mathbf{X}^T\\mathbf{W}\\mathbf{X}\n",
    "\\end{align}\n",
    "其中 $\\mathbf{X}$ 是 $N\\times(p+1)$ 维矩阵，\n",
    "\\begin{align}\n",
    "\\mathbf{P} =& \\left[p(x_0;\\beta),\\ p(x_1;\\beta),\\ p(x_2;\\beta),\\ \\cdots,\\ p(x_N;\\beta)  \\right]^T \\\\\n",
    "\\mathbf{W} =& diag \\left[p(x_0;\\beta)(1-p(x_0;\\beta)) \\quad p(x_1;\\beta)(1-p(x_1;\\beta)) \\quad p(x_2;\\beta)(1-p(x_2;\\beta))\\quad \\cdots \\quad p(x_N;\\beta)(1-p(x_N;\\beta)) \\right]\n",
    "\\end{align}\n",
    "最后牛顿迭代可以写成\n",
    "\\begin{align}\n",
    "\\beta^{new} = \\beta^{old} + (\\mathbf{X}^T\\mathbf{W}\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{y}-\\mathbf{p}) = (\\mathbf{X}^T\\mathbf{W}\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{W}\\mathbf{z}\n",
    "\\end{align}\n",
    "where\n",
    "\\begin{align}\n",
    "\\mathbf{z} = \\mathbf{X} \\beta^{old} + \\mathbf{W}^{-1}(\\mathbf{y}-\\mathbf{p}).\n",
    "\\end{align}\n",
    "上面的迭代算法被称为 *iteratively reweighted least squares* (IRLS)，因为每一步相当于求解一个 weighted least squares 问题：\n",
    "\\begin{align}\n",
    "\\beta^{new}\\leftarrow \\arg \\min_{\\beta} (\\mathbf{z} - \\mathbf{X}\\beta )^T\\mathbf{W} (\\mathbf{z} - \\mathbf{X}\\beta ).\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
