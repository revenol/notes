{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splines\n",
    "Chapter 5 Basis Expansions and Regularization\n",
    "\n",
    "*这一章节其实是关于实时数据/信号的拟合，和我关注的 AI 技术并不是那么紧密（暂时这么认为），并没有真正理解技术内涵，只是大致看了个概念。*\n",
    "\n",
    "前面学习了 Linear Regression/Classification，但是现实数据大部分不是线性的。如果数据不是线性的，又不是周期的，很难用一个函数表达出，于是就把它分为几段 knots。根据knots的数量，可以有不同的方法。之前的 linear 分析，相当于没有 knot。另一方面，我们可以对任意函数进行泰勒展开，而线性方法相当于一阶泰勒展开。理论上，只要我们进行无穷的泰勒展开，我们就能表达任意的函数/数据了。这也是 basis expansion 的根基。\n",
    "\n",
    "理论很美好，然而人类只能处理线性。对于非线性的，我们对输入变量进行转化，引入 $h_m(X)$，$m=1,2,\\cdots,M$。我们可以处理的线性模型为\n",
    "$$\n",
    "\\begin{align}\n",
    "f(X) = \\sum_{m=1}^{M}\\beta_m h_m(X),\n",
    "\\end{align}\n",
    "$$\n",
    "称为 $X$ 的 linear basis expansion。$h_m(X)$ 的常用形式有\n",
    "- $h_m(X) = X_m$, with $m=1,2,\\cdots,p$， 这个就是经典的 linear model \n",
    "- $h_m(X) = X_j^2$ or $h_m(X) = X_j X_k$，二阶泰勒展开\n",
    "- $h_m(X) = \\log(X_j), \\sqrt{X_j}, \\|X\\|,\\cdots$，非线性模型\n",
    "- $h_m(X) = {I}({L}_m\\leq X_k \\leq U_m)$，piecewise constant contribution for $X_k$。\n",
    "\n",
    "每一个 $h_m$ 相当于 $X$ 的一个 basis，显然 basis expansion 越多效果越好。在实际处理数据的时候，不可避免的会遇到高复杂度，解决方案有三种：\n",
    "- Restriction methods, 严格限制 $h_m$ 的数量，当然会牺牲效果。\n",
    "- Selection methods, 选择最重要的 $h_m$，相当于 Principal Component Regression (PCR)。\n",
    "- Regularization methods, 给每一个 $h_m$ 加上系数权重，相当于 Ridge Regression。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "1. Piecewise Polynomials and Splines。对应上面提到的 Restriction methods。把数据分割成多个段 splines，然后在每一个段区间用多项式拟合。然后经验告诉我们，$M=1,2,4$ 的时候效果已经可以了。\n",
    "*在我看来，这个是一个世纪以前计算能力不够，人类能够到达的极限。就和大多数情况下，线性模型就够用了一个道理。该方法只是给人类一个直观的视觉解释，对于 AI 来说没有意义。（当然，统计的好多工作就是为了提供 interpretation， 而 AI 根本不需要这些人类能够理解的 interpretation。）*\n",
    "\n",
    "1. Smoothing Splines。对应上面提到的 Regularization methods。引入了 *smoothing parameter*，其实就是个 panalized residual sum of squares\n",
    "$$\n",
    "\\begin{align}\n",
    "RSS(f,\\lambda) = \\sum_{i=1}^{N}\\{y_i - f(x_i)\\}^2 + \\lambda \\int\\{f''(t)\\}^2dt\n",
    "\\end{align}\n",
    "$$\n",
    "$\\lambda$ 的选择，是 Bias-Variance 的 tradeoff。\n",
    "\n",
    "1. Reproducing Kernel Hilbert Spaces (RKHS)。把变量转换到希尔伯特正交空间，然后就可以做傅里叶变换了，比如信号处理常用的 FFT。算是 basis expansion 最重要的一种。以后会有章节专门讲 kernel，略过。\n",
    "\n",
    "1. Wavelet Smoothing 小波平滑。同时基于时域与频域的分析。对于通信背景的我，熟悉频域分析，这个也就掠过了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 具体内容\n",
    "无。*好吧，我对整个章节内容抱有偏见，因为用到的技术太复杂，而辛苦得到的结果，如果仅仅是为人类提供 interpretation，我是不会去处理的。如果后面的章节需要用到这些基础知识，之后再来补充吧。*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
