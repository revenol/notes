{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "*Chpater 3. Linear Methods for Regression*\n",
    "\n",
    "线性表达式： \n",
    "$$\n",
    "\\begin{align}\n",
    "f(X)=\\beta_0 + \\sum_{j=1}^{p}X_j\\beta_j\n",
    "\\end{align}\n",
    "$$\n",
    "其中，输入$X_j$有$N$个观察值，共有$p$维；$\\beta_0$是引入的intercept，又称为bias.\n",
    "写成矩阵形式\n",
    "$$\n",
    "\\begin{align}\n",
    "f(X)=\\mathbf{X}\\beta,\n",
    "\\end{align}\n",
    "$$\n",
    "其中$\\mathbf{X}$是$N\\times(p+1)$矩阵。\n",
    "\n",
    "## 内容概览\n",
    "- Least Squares。常被称为Ordinary Least Squares (OLS)，即最小二乘法。目标是最小化residual sum of squares:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{RSS}(\\beta)=&\\sum_{i=1}^{N}(y_i-f(x_i))^2=\\sum_{i=1}^{N}(y_i-\\beta_0-\\sum_{j=1}^{p}x_{ij}\\beta_j)^2=(\\mathbf{y}-\\mathbf{X}\\beta)^T(\\mathbf{y}-\\mathbf{X}\\beta) \\\\\n",
    "\\hat{\\beta}=&\\arg \\min_{\\beta}\\textrm{RSS}(\\beta) =(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "$$\n",
    "- Ridge Regression。目标是最小化penalized residual sum of squares with $L_2$ penalty $\\sum_{i=1}^{p}\\beta_j^2$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{RSS}(\\lambda) =&  \\sum_{i=1}^{N}(y_i-\\beta_0-\\sum_{j=1}^{p}x_{ij}\\beta_j)^2+\\lambda\\sum_{i=1}^{p}\\beta_j^2 =(\\mathbf{y}-\\mathbf{X}\\beta)^T(\\mathbf{y}-\\mathbf{X}\\beta) + \\lambda\\beta^T\\beta \\\\\n",
    "\\hat{\\beta}^{\\textrm{ridge}} =&\\arg \\min_{\\beta}\\textrm{RSS}(\\lambda) = (\\mathbf{X}^T\\mathbf{X}+\\lambda \\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "$$\n",
    "- Least Absolute Shrinkage and Selection Operator (Lasso)。目标是最小化penalized residual sum of squares with $L_1$ penalty $\\sum_{i=1}^{p}\\left|\\beta_j\\right|$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{RSS}(\\lambda) =  \\tfrac{1}{2}\\sum_{i=1}^{N}(y_i-\\beta_0-\\sum_{j=1}^{p}x_{ij}\\beta_j)^2+\\lambda\\sum_{i=1}^{p}\\left|\\beta_j\\right|=\\tfrac{1}{2}\\left\\|\\mathbf{y}-\\mathbf{X}\\beta)\\right\\|_2^2 + \\lambda \\left\\|\\beta\\right\\|_1\n",
    "\\end{align}\n",
    "$$\n",
    "由于$L_1$ Lasso penalty $\\sum_{i=1}^{p}\\left|\\beta_j\\right|$是非线性，没有closed-form expressions，可以通过quadratic programming problem动态求解。对于给定的$\\lambda$，如果$\\textrm{RSS}(\\beta)$关于变量$\\beta$可导的话，可以得到stationary conditions\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{x}_j^T(\\mathbf{y}-\\mathbf{X}\\beta)=\\lambda\\cdot sign(\\beta_j).\n",
    "\\end{align}\n",
    "$$\n",
    "- Generalied Ridge and Lasso Regression with $L_p$ penalty. \n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{\\beta} =\\arg \\min_{\\beta} \\left\\{ \\sum_{i=1}^{N}(y_i-\\beta_0-\\sum_{j=1}^{p}x_{ij}\\beta_j)^2+\\lambda\\sum_{i=1}^{p}\\left|\\beta_j\\right|^p \\right \\}\n",
    "\\end{align}\n",
    "$$\n",
    "The _Elastic-net_ penality 是Ridge和Lasso的组合，类似与$p\\in[1,2]$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\lambda\\sum_{j=1}^{p}\\left(\\alpha \\beta_j^2 + (1-\\alpha)\\left|\\beta_j\\right|\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "- Least Angle Regression (LAR)。 与Forward stepwise regression类似，每次把一个与当前residual $\\mathbf{r}=\\mathbf{y}-\\bar{\\mathbf{y}}$最相关variable $\\mathbf{x}_j$加入到_active set_中。但是，LAR并不是一次性把整个variable都增加进来，而是通过一个系数加入一小部分，然后在每一步调整系数。由于系数缓慢调整，避免了variable被整个加入又被删除的累赘操作。LAR在性能上与Lasso相同，并且为Lasso提供了快速算法实现。\n",
    "- Principal Components Regression (PCR)。系统的输入$X_j$共有$p$维，当$p$较大时，计算复杂度高、准确率低，重中筛选出有效相关的$M$维线性组合$Z_m$，然后基于$Z_m$做regression.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{y}}_{(M)}^{pcr} =& \\bar{y}\\mathbf{1}+\\sum_{m=1}^{M}\\hat{\\theta}_m\\mathbf{z}_m , \\quad\n",
    "\\hat{\\theta}_m = \\frac{\\left<\\mathbf{z}_m, \\mathbf{y}\\right>}{\\left<\\mathbf{z}_m, \\mathbf{z}_m\\right>}, \\quad\\mathbf{z}_m=\\mathbf{X}v_m \\\\\n",
    "\\hat{\\beta}^{pcr}(M)= & \\sum_{m=1}^{M}\\hat{\\theta}_mv_m\n",
    "\\end{align}\n",
    "$$\n",
    "PCR与Ridge Regression都是基于输入变量矩阵的principal components做regression，前者忽略了$p-M$个最小eigenvalue components，后者shrinks更多的principal components系数。\n",
    "- Partial Least Squares (PLS)。PLS与PCR类似，不同之处是PLS中$\\mathbf{z}_m$的生成同时基于$\\mathbf{x}$和$\\mathbf{y}$。\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{z}_m = \\sum_{j=1}^{p}\\left<\\mathbf{x}_j^{(m-1)},\\mathbf{y}\\right>\\mathbf{x}_j^{(m-1)}\n",
    "\\end{align}\n",
    "$$\n",
    "PCR仅与输入有关，容易出错；PLS同时考虑输入和输出，效果较稳定。\n",
    "\n",
    "\n",
    "## 具体内容\n",
    "\n",
    "\n",
    "### Least Squares\n",
    "\n",
    "\n",
    "#### 最优$\\beta$    \n",
    "RSS可以写成矩阵形式\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{RSS}(\\beta)=(\\mathbf{y}-\\mathbf{X}\\beta)^T(\\mathbf{y}-\\mathbf{X}\\beta),\n",
    "\\end{align}\n",
    "$$\n",
    "其中$\\mathbf{y}$是一个$N$-vector。求导：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial RSS}{\\partial \\beta}=&\\frac{\\partial}{\\partial \\beta}\\left( \\mathbf{y}^T\\mathbf{y}-2\\beta^T\\mathbf{X}^T\\mathbf{y}+\\beta^T\\mathbf{X}^T\\mathbf{X}\\mathbf{y} \\right)=-2\\mathbf{X}^T(\\mathbf{y}-\\mathbf{X}\\beta). \\\\\n",
    "\\frac{\\partial^2 RSS}{\\partial \\beta \\partial \\beta^T}=&2\\mathbf{X}^T\\mathbf{X}\n",
    "\\end{align}\n",
    "$$\n",
    "令一阶导数等于0，可以求得最优解\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### 最优$\\beta$计算    \n",
    "1. 对$\\mathbf{X}$做$QR$分解:\n",
    "$\\mathbf{X}=\\mathbf{Q}\\mathbf{R}$。其中$\\mathbf{Q}$是$N\\times(p+1)$正定矩阵，满足$\\mathbf{Q}^T\\mathbf{Q}=\\mathbf{I}$；$\\mathbf{R}$是$(p+1)\\times (p+1)$上三角矩阵。\n",
    "2. 计算$\\hat{\\beta}$：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\beta} = (\\mathbf{R}^T\\mathbf{Q}^T\\mathbf{Q}\\mathbf{R})^{-1}\\mathbf{R}^T\\mathbf{Q}^T\\mathbf{y}= (\\mathbf{R}^T\\mathbf{R})^{-1}\\mathbf{R}^T\\mathbf{R}\\mathbf{R}^{-1}\\mathbf{Q}^T\\mathbf{y}=\\mathbf{R}^{-1}\\mathbf{Q}^T\\mathbf{y}.\n",
    "\\end{align}\n",
    "$$\n",
    "3. 估计$\\hat{y}$：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} = \\mathbf{Q}\\mathbf{Q}^T\\mathbf{y}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### 缺陷\n",
    "- _prediction accuracy_:在bias和variance的trade-off中，OLS通常有低bias和高variance。\n",
    "- _interpretation_:考虑了所有变量维度，但其中某些维度可能是干扰项目，与实际$Y$无关。OLS不能很好的提供具有物理意义的解释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
